{"cells":[{"cell_type":"markdown","metadata":{"id":"XFCWflMKcmH-"},"source":["# New Game Play"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3nc7NVffcmIA"},"outputs":[],"source":["import copy\n","import random\n","from json.encoder import INFINITY\n","import pandas as pd\n","import math\n","\n","seed = 37\n","\n","randint = random.randint\n","random.seed(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1nNG_hoEcmIB"},"outputs":[],"source":["def end_game(state):\n","    return sum(state['board']) == 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ewp4cJaWcmIB"},"outputs":[],"source":["def assign_reward(reward, new_reward):\n","    return [reward[0] + new_reward[0], reward[1] + new_reward[1], new_reward[2]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-6uvBrocmIC"},"outputs":[],"source":["def terminate_loop(state):\n","    state['board'] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","    reward = (0,0,0)\n","\n","    return (state, reward)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hpMes3RmcmIC"},"outputs":[],"source":["def print_game_play(state, reward, new_starting_position):\n","    print('new_starting_position: ', new_starting_position)\n","    print('state: ', state)\n","    print('reward: ', reward)\n","    print('--------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iKZYcAyRcmIC"},"outputs":[],"source":["def four_left(state, reward):\n","    state['board'] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","\n","    if reward[2] == 0:\n","        return  (4,0,0)\n","\n","    if reward[2] == 1:\n","        return  (0,4,1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yKWqiGnmcmIC"},"outputs":[],"source":["def get_valid_actions(arr):\n","    new_arr = []\n","    for i,a in enumerate(arr):\n","        if a != 0:\n","            new_arr.append(i)\n","    return new_arr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7VZEM6EEcmID"},"outputs":[],"source":["def generate_action(state,agent_1, agent_2):\n","   if state['current_player'] == 0:\n","      func = agent_1['func']\n","      arg = agent_1['arg']\n","      return func(state, arg)\n","\n","   if state['current_player'] == 1:\n","      func = agent_2['func']\n","      arg = agent_2['arg']\n","      return func(state, arg)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HYBp52LHcmID"},"outputs":[],"source":["def is_valid_actions(state):\n","\n","    if state['current_player'] == 0:\n","          val = state['board'][0:state['player_territory'][1]]\n","          return sum(val)\n","\n","    if state['current_player'] == 1:\n","          val = state['board'][state['player_territory'][1]:12]\n","          return sum(val)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ib60kR2CcmID"},"outputs":[],"source":["def get_reward(stones_in_pit, board, position, stone, current_player, player_territory):\n","    board[position] = 0\n","    if stone == stones_in_pit-1 and position < player_territory[1] and current_player == 1:\n","        return (0,4,1)\n","\n","    if stone == stones_in_pit-1 and position >= player_territory[1] and current_player == 0:\n","        return  (4,0,0)\n","\n","    if position < player_territory[1]:\n","        return  (4,0,0)\n","        # return (0,4,1)\n","\n","    else:\n","        return (0,4,1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mPzgHWDocmID"},"outputs":[],"source":["def is_illegal_move(state, action):\n","    board = state['board']\n","    current_player = state['current_player']\n","    player_territory = state['player_territory']\n","    stones_in_pit = board[action]\n","\n","    # is pit empty\n","    if stones_in_pit == 0:\n","        # print('pit is empty')\n","        return True\n","\n","    # is pit not in player one's territory\n","    if current_player == 0 and action >= player_territory[1]:\n","        # print(\"pit is not in player 1's territory\")\n","        return True\n","\n","    # is pit not in player two's territory\n","    if current_player == 1 and action <  player_territory[1]:\n","        # print(\"pit is not in player 2's territory\")\n","        return True\n","\n","    return False\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LNBrmPw4cmID"},"outputs":[],"source":["def session(state, starting_position,latest_winner ):\n","    new_state = copy.deepcopy(state)\n","\n","    board = new_state['board']\n","    current_player = new_state['current_player']\n","    player_territory = new_state['player_territory']\n","    stones_in_pit = board[starting_position]\n","    board[starting_position] = 0\n","    reward = [0,0,latest_winner]\n","\n","\n","    for stone in range(stones_in_pit):\n","        future_position = (stone + starting_position + 1) % 12\n","        board[future_position] += 1\n","\n","        if board[future_position] == 4:\n","             new_reward = get_reward(stones_in_pit, board, future_position, stone, current_player, player_territory)\n","             reward = assign_reward(reward, new_reward)\n","\n","    return (new_state, reward, future_position)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E_RXc1D4cmIE"},"outputs":[],"source":["def play(state, action, show=False):\n","    reward = [0,0,0]\n","    max_rez = 100\n","    rez = 0\n","    # print(is_illegal_move(state, action))\n","    if is_illegal_move(state, action):\n","        new_state = {\n","        'board' : [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n","        'current_player': state['current_player'],\n","        'player_territory': (0,6)}\n","        r = new_state['current_player']\n","        reward = [r*400,((r+1)%2)*400,0]\n","        return (new_state, reward)\n","\n","    state, new_reward, new_starting_position = session(state, action, reward[2])\n","    reward = assign_reward(reward, new_reward)\n","    if show:\n","        print_game_play(state, reward, new_starting_position)\n","\n","    board = state['board']\n","    stones_in_pit = board[new_starting_position]\n","\n","    while stones_in_pit > 1:\n","        state, new_reward, new_starting_position = session(state,new_starting_position,reward[2])\n","        reward = assign_reward(reward, new_reward)\n","        if show:\n","            print_game_play(state, reward, new_starting_position)\n","\n","        board = state['board']\n","        stones_in_pit = board[new_starting_position]\n","\n","        if rez > max_rez:\n","            state, reward = terminate_loop(state)\n","            return (state, reward)\n","\n","        rez += 1\n","\n","\n","    state['current_player'] = +(not state['current_player'])\n","\n","    if sum(state['board']) <= 4:\n","        new_reward = four_left(state, reward)\n","        reward = assign_reward(reward, new_reward)\n","\n","    if not is_valid_actions(state):\n","        state['current_player'] = +(not state['current_player'])\n","\n","    return (state, reward)"]},{"cell_type":"markdown","metadata":{"id":"2mzdjbUkcmIE"},"source":["## Simulation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CaGotfb5cmIE"},"outputs":[],"source":["def simulate_game(state,  agent_1, agent_2,show=False):\n","    state = copy.deepcopy(state)\n","    reward = [0,0,0]\n","    path = []\n","\n","    while True:\n","        action = generate_action(state, agent_1, agent_2)\n","\n","        if is_illegal_move(state, action):\n","            continue\n","\n","        state, new_reward = play(state, action)\n","        reward = assign_reward(reward, new_reward)\n","        if show:\n","            print_game_play(state, reward, action)\n","\n","        if end_game(state):\n","            break\n","        path.append(action)\n","\n","    return  (reward, path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qsc1-WGVcmIE"},"outputs":[],"source":["def get_valid_actions_mct(state):\n","    board = state['board']\n","    territory = state['player_territory'][1]\n","    current_player = state['current_player']\n","    valid_actions = []\n","    for i,a in enumerate(board):\n","        if current_player == 0 and i < territory and a != 0:\n","                    valid_actions.append(i)\n","        if current_player == 1 and i >= territory and a != 0:\n","                    valid_actions.append(i)\n","    return valid_actions"]},{"cell_type":"markdown","metadata":{"id":"ZmUR1hNecmIE"},"source":["## Agents"]},{"cell_type":"markdown","metadata":{"id":"wYha0fotcmIE"},"source":["### Random Agent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dBJfEKh_cmIE"},"outputs":[],"source":["def random_agent(state, arg=None):\n","        valid_actions = get_valid_actions_mct(state)\n","        return valid_actions[randint(0,len(valid_actions)-1)]"]},{"cell_type":"markdown","metadata":{"id":"sg14brD7cmIE"},"source":["### Minimax Agent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wa6kW2KxcmIE"},"outputs":[],"source":["\n","def minimax_agent(state, arg):\n","    # print(state)\n","    max_dept = arg['max_dept']\n","    reward = [0,0,0]\n","    all_game_state = []\n","    all_game_state.append((state, reward))\n","\n","    if state['current_player'] == 0:\n","        best_score = -INFINITY\n","        for action in range(state['player_territory'][1]):\n","            temp_state, reward =  copy.deepcopy(all_game_state[-1])\n","            # temp_state['path'].append(child)\n","            game_state, new_reward = play(temp_state, action)\n","            reward = assign_reward(reward, new_reward)\n","            all_game_state.append((game_state, reward))\n","            score = minimax(game_state, False, all_game_state, reward, max_dept, -INFINITY, INFINITY)\n","            all_game_state.pop()\n","\n","            if score > best_score:\n","                best_score = score\n","                move = action\n","\n","    else:\n","        best_score = INFINITY\n","        for action in range(state['player_territory'][1],12):\n","            temp_state, reward =  copy.deepcopy(all_game_state[-1])\n","            game_state, new_reward = play(temp_state, action)\n","            reward = assign_reward(reward, new_reward)\n","            all_game_state.append((game_state, reward))\n","            score = minimax(game_state, True,all_game_state,reward,max_dept,-INFINITY,INFINITY)\n","            all_game_state.pop()\n","\n","            if score < best_score:\n","                best_score = score\n","                move = action\n","    # print('best_score',best_score)\n","    return move\n","\n","\n","\n","def minimax(game_state, is_maximizing,all_game_state,reward,max_dept,alpha,beta ):\n","    max_dept -= 1\n","    # game_state['current_player'] = current_player\n","\n","    if(max_dept <= 0 or  end_game(game_state)):\n","        return reward[0]-reward[1]\n","\n","    if is_maximizing:\n","        best_score = -INFINITY\n","\n","        for action in range(game_state['player_territory'][1]):\n","            temp_state, reward =  copy.deepcopy(all_game_state[-1])\n","            game_state, new_reward = play(temp_state, action)\n","            reward = assign_reward(reward, new_reward)\n","            all_game_state.append((game_state, reward))\n","            score = minimax(game_state, False,all_game_state,reward,max_dept,alpha,beta)\n","            all_game_state.pop()\n","            best_score = max(score, best_score)\n","            alpha = max(alpha,score)\n","\n","            if beta <= alpha:\n","                break\n","\n","        return best_score\n","\n","    else:\n","        best_score = INFINITY\n","\n","        for action in range(game_state['player_territory'][1],12):\n","            temp_state, reward =  copy.deepcopy(all_game_state[-1])\n","            # print(temp_state)\n","            game_state, new_reward = play(temp_state, action)\n","            reward = assign_reward(reward, new_reward)\n","            all_game_state.append((game_state, reward))\n","            score = minimax(game_state, True,all_game_state,reward,max_dept,alpha,beta)\n","            all_game_state.pop()\n","            best_score = min(score, best_score)\n","            beta = min(beta,score)\n","\n","            if beta <= alpha:\n","                break\n","\n","        return best_score\n"]},{"cell_type":"markdown","metadata":{"id":"PYphcrUOcmIE"},"source":["## Monter Carlo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YShC3rPrcmIE"},"outputs":[],"source":["class Node():\n","    def __init__(self,state,root_current_player, action=None,parent_node=None):\n","        self.parent_node = parent_node\n","        self.action = action\n","        self.legal_actions = []\n","        self.total_score = 0\n","        self.visit_count = 0\n","        self.expanded = False\n","        self.children = None\n","        self.state = state\n","        self.root_current_player = root_current_player\n","\n","    def update_result(self, reward):\n","        self.total_score += reward\n","        self.visit_count += 1\n","\n","    def printer(self):\n","        print('parent_node: ',self.parent_node)\n","        print('action: ',self.action)\n","        print('legal_actions: ',self.legal_actions)\n","        print('total_score: ',self.total_score)\n","        print('visit_count: ',self.visit_count)\n","        print('expanded: ',self.expanded)\n","        print('children: ',self.children)\n","        print('root_current_player: ',self.root_current_player)\n","        print('state: ',self.state)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-T-lUWK5cmIF"},"outputs":[],"source":["def expand(node):\n","    state = node.state\n","    actions = get_valid_actions_mct(state)\n","    node.legal_actions = actions\n","    node.expanded = True\n","    root_current_player = node.root_current_player\n","    node.children = {}\n","\n","    for action in actions:\n","        new_state, reward = play(state, action)\n","        node.children[action] = Node(new_state,root_current_player,action,node)\n","    return node"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RjdcuDfmcmIF"},"outputs":[],"source":["def resources_left( max_iterations, iterations):\n","    return max_iterations > iterations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aD-5aiEdcmIF"},"outputs":[],"source":["def ucb(constant = 2, total_score = 0, number_of_parent_visits = 0, number_of_visits = 0):\n","    if number_of_visits == 0:\n","        return INFINITY\n","    avg_score = total_score / number_of_visits\n","\n","    return (avg_score +(constant*math.sqrt(math.log(number_of_parent_visits)/number_of_visits)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GEvttoSQcmIF"},"outputs":[],"source":["def select(node):\n","    actions = node.legal_actions\n","\n","    if len(actions) == 0:\n","        return None\n","\n","    best_score = -INFINITY\n","    for action in actions:\n","        child = node.children[action]\n","        total_score = child.total_score\n","        parent_visit_count = node.visit_count\n","        visit_count = child.visit_count\n","        score = ucb(2,total_score, parent_visit_count, visit_count )\n","\n","        if score > best_score:\n","            best_score = score\n","            best_action = action\n","\n","    return best_action"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"06z0vtTgcmIF"},"outputs":[],"source":["# function for the result of the simulation\n","def rollout(node):\n","\n","    state = node.state\n","\n","    player_1 = {\n","    'func': random_agent,\n","    'arg': {}\n","    }\n","\n","    player_2 = {\n","    'func': random_agent,\n","    'arg': {}\n","    }\n","\n","    if end_game(state):\n","            return 0\n","#     print('current_player',state['current_player'])\n","#     print('root_current_player',node.root_current_player)\n","    reward, path = simulate_game(state, player_1, player_2)\n","#     print(reward[node.root_current_player])\n","\n","    if reward[node.root_current_player] > 24:\n","        results = 1\n","    if reward[node.root_current_player] < 24:\n","        results = 0\n","    if reward[node.root_current_player] == 24:\n","        results = -1\n","\n","    return  results\n","    # return  reward[node.root_current_player]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5blcCPqicmIF"},"outputs":[],"source":["def best_child(node):\n","    actions = node.legal_actions\n","    highest_visit = -INFINITY\n","    for action in actions:\n","        child = node.children[action]\n","        visit_count = child.visit_count\n","        if visit_count > highest_visit:\n","            highest_visit = visit_count\n","            best_action = action\n","    return best_action"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ifoDOhMgcmIF"},"outputs":[],"source":["def back_propagation(node, result):\n","    node.update_result(result)\n","    parent_node =node.parent_node\n","\n","    if parent_node == None:\n","        return\n","\n","    back_propagation(parent_node, result)"]},{"cell_type":"markdown","metadata":{"id":"_fzZ-QF3cmIF"},"source":["## Monte Carlo Tree Search (MCTS) Agent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7N_OK0ihcmIF"},"outputs":[],"source":["def mcts_agent(state,arg):\n","    max_iterations = arg['max_iterations']\n","    root = Node(state,state['current_player'])\n","    expand(root)\n","    node = root\n","    i = 0\n","\n","    while resources_left(max_iterations, i):\n","        i += 1\n","        while node.children:\n","            action = select(node)\n","\n","            if action == None:\n","                break\n","            child = node.children[action]\n","            node = child\n","            # node.printer()\n","\n","        if node.visit_count == 0:\n","            result = rollout(node)\n","            back_propagation(node, result)\n","            node = root\n","        else:\n","            expand(node)\n","            action = select(node)\n","            if action == None:\n","                break\n","            child = node.children[action]\n","            node = child\n","            result = rollout(node)\n","            back_propagation(node, result)\n","            node = root\n","\n","    return best_child(root)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"54499u99cmIF"},"outputs":[],"source":["state = {\n","   # 'board' :[6, 6, 2, 7, 1, 6, 1, 6, 6, 6, 0, 1],\n","   'board' :[4,4,4,4,4,4,4,4,4,4,4,4],\n","   'current_player': 0,\n","   'player_territory': (0,6)\n","}\n","\n","t = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0}\n","for i in range(10):\n","   action, node = mcts_agent(state,{'max_iterations': 1000})\n","   t[action] += 1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P_zgtnUUcmIF"},"outputs":[],"source":["t"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4-l1sc7EcmIG"},"outputs":[],"source":["ns = node.children[7]\n","for child in ns.children:\n","    print(ns.children[child].printer())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n8cr8O-SE2vx"},"outputs":[],"source":["def format_state(state, action):\n","    board = state['board']\n","    data = []\n","    for i,pit in enumerate(board):\n","        num = str(i)\n","        data[f'pit {num}'] = pit\n","    data['current_player'] = state['current_player']\n","    data['player_territory'] = state['player_territory'][1]\n","    data['action'] = action\n","    return [data]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pbl5uJ1IE2vx"},"outputs":[],"source":["state = {\n","   'board' :[4,4,4,4,4,4,4,4,4,4,4,4],\n","   'current_player': 0,\n","   'player_territory': (0,6)\n","}\n","\n","action = 0\n","format_state(state, action)"]},{"cell_type":"markdown","metadata":{"id":"9UsY5drsE2vy"},"source":["## Elo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4AmmIfFiE2v1"},"outputs":[],"source":["def update_score(actual_result, expected_result):\n","    k = 20\n","    return k * (actual_result-expected_result)\n","\n","def expected_score(elo_1,elo_2):\n","    diff = elo_2 - elo_1\n","    a = diff/400\n","    d = 1 + 10**a\n","    return 1/d\n","\n","def calculate_elo(elo_0,elo_1,result):\n","    expected_result = expected_score(elo_0,elo_1)\n","    change = update_score(result, expected_result)\n","\n","    new_elo_0 = elo_0 + change\n","    new_elo_1 = elo_1 - change\n","\n","    return (new_elo_0, new_elo_1)"]},{"cell_type":"markdown","metadata":{"id":"edZ9zYTiE2v1"},"source":["## Test"]},{"cell_type":"markdown","metadata":{"id":"zMrCCCh3E2v1"},"source":["### Agents Details"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PDWmrBWbE2v2"},"outputs":[],"source":["random_agent_details = {\n","   'func': random_agent,\n","   'arg': {},\n","   'name': 'random_agent',\n","   'elo': 1200\n","}\n","\n","mcts_agent_details_10 = {\n","   'func': mcts_agent,\n","   'arg': {\n","      'max_iterations': 10,\n","   },\n","   'name': 'mcts_agent_10',\n","   'elo': 1200\n","}\n","\n","mcts_agent_details_100 = {\n","   'func': mcts_agent,\n","   'arg': {\n","      'max_iterations': 100,\n","   },\n","   'name': 'mcts_agent_100',\n","   'elo': 1200\n","}\n","\n","mcts_agent_details_1000 = {\n","   'func': mcts_agent,\n","   'arg': {\n","      'max_iterations': 1000,\n","   },\n","   'name': 'mcts_agent_1000',\n","   'elo': 1200\n","}\n","\n","minimax_agent_details_3 = {\n","   'func': minimax_agent,\n","   'arg': {\n","      'max_dept': 3,\n","   },\n","   'name': 'minimax_agent_dept_3',\n","   'elo': 1200\n","}\n","\n","minimax_agent_details_6 = {\n","   'func': minimax_agent,\n","   'arg': {\n","      'max_dept': 6,\n","   },\n","   'name': 'minimax_agent_dept_6',\n","   'elo': 1200\n","}\n","\n","minimax_agent_details_9 = {\n","   'func': minimax_agent,\n","   'arg': {\n","      'max_dept': 9,\n","   },\n","   'name': 'minimax_agent_dept_9',\n","   'elo': 1200\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M7OBWaBXE2v2"},"outputs":[],"source":["def match_up(origin_state,number_of_games, player_1, player_2 ):\n","   results = {\n","         'player_1_name': player_1['name'],\n","         'player_2_name': player_2['name'],\n","         'player_1_wins': 0,\n","         'player_2_wins': 0,\n","         'ties': 0,\n","         'player_1_elo': player_1['elo'],\n","         'player_2_elo': player_2['elo'],\n","      }\n","\n","   for i in range(number_of_games):\n","      reward, path = simulate_game(origin_state, player_1, player_2)\n","      # paths.append([reward,path])\n","\n","      if reward[0] > reward[1]:\n","         results['player_1_wins'] += 1\n","         elo_0, elo_1 = calculate_elo(player_1['elo'],player_2['elo'],1)\n","      elif reward[0] < reward[1]:\n","         results['player_2_wins'] += 1\n","         elo_0, elo_1 = calculate_elo(player_1['elo'],player_2['elo'],0)\n","      elif reward[0] == reward[1]:\n","         results['ties'] += 1\n","         elo_0, elo_1 = calculate_elo(player_1['elo'],player_2['elo'],0.5)\n","\n","      player_1['elo'],player_2['elo'] = (elo_0, elo_1)\n","\n","   results['player_1_elo'] = player_1['elo']\n","   results['player_2_elo'] = player_2['elo']\n","   results['player_1_wins'] = results['player_1_wins']/number_of_games * 100\n","   results['player_2_wins'] = results['player_2_wins']/number_of_games * 100\n","   results['ties'] = results['ties']/number_of_games * 100\n","\n","   return results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16966657,"status":"ok","timestamp":1729296387674,"user":{"displayName":"Nwachukwu Ujubuonu","userId":"09987348326723992864"},"user_tz":-60},"id":"hMHcAEg9cmIG","outputId":"5dfafb0f-45c7-472f-ce3b-e94914ba7b71"},"outputs":[{"name":"stdout","output_type":"stream","text":["random_agent wins: 0.0\n","minimax_agent_dept_3 wins: 98.0\n","ties:  2.0\n","random_agent elo:  940.7576240420559\n","minimax_agent_dept_3 elo:  1459.2423759579453\n","-----------------------------------------\n","random_agent wins: 0.0\n","minimax_agent_dept_6 wins: 98.0\n","ties:  2.0\n","random_agent elo:  803.6745837740348\n","minimax_agent_dept_6 elo:  1337.0830402680217\n","-----------------------------------------\n","random_agent wins: 0.0\n","minimax_agent_dept_9 wins: 100.0\n","ties:  0.0\n","random_agent elo:  701.4395706632646\n","minimax_agent_dept_9 elo:  1302.23501311077\n","-----------------------------------------\n","random_agent wins: 15.0\n","mcts_agent_10 wins: 65.0\n","ties:  20.0\n","random_agent elo:  825.023880098059\n","mcts_agent_10 elo:  1076.4156905652064\n","-----------------------------------------\n","random_agent wins: 16.0\n","mcts_agent_100 wins: 62.0\n","ties:  22.0\n","random_agent elo:  907.5428721280138\n","mcts_agent_100 elo:  1117.4810079700449\n","-----------------------------------------\n","random_agent wins: 19.0\n","mcts_agent_1000 wins: 56.99999999999999\n","ties:  24.0\n","random_agent elo:  961.5814682506651\n","mcts_agent_1000 elo:  1145.9614038773493\n","-----------------------------------------\n","minimax_agent_dept_3 wins: 87.0\n","random_agent wins: 5.0\n","ties:  8.0\n","minimax_agent_dept_3 elo:  1425.4850264393715\n","random_agent elo:  995.3388177692391\n","-----------------------------------------\n","minimax_agent_dept_3 wins: 0.0\n","minimax_agent_dept_6 wins: 100.0\n","ties:  0.0\n","minimax_agent_dept_3 elo:  1119.3429427813016\n","minimax_agent_dept_6 elo:  1643.2251239260916\n","-----------------------------------------\n","minimax_agent_dept_3 wins: 0.0\n","minimax_agent_dept_9 wins: 100.0\n","ties:  0.0\n","minimax_agent_dept_3 elo:  934.8069486462083\n","minimax_agent_dept_9 elo:  1486.7710072458628\n","-----------------------------------------\n","minimax_agent_dept_3 wins: 45.0\n","mcts_agent_10 wins: 6.0\n","ties:  49.0\n","minimax_agent_dept_3 elo:  1088.5725683776172\n","mcts_agent_10 elo:  922.6500708337988\n","-----------------------------------------\n","minimax_agent_dept_3 wins: 56.00000000000001\n","mcts_agent_100 wins: 13.0\n","ties:  31.0\n","minimax_agent_dept_3 elo:  1196.9285371894596\n","mcts_agent_100 elo:  1009.125039158202\n","-----------------------------------------\n","minimax_agent_dept_3 wins: 46.0\n","mcts_agent_1000 wins: 2.0\n","ties:  52.0\n","minimax_agent_dept_3 elo:  1253.3234382471148\n","mcts_agent_1000 elo:  1089.566502819694\n","-----------------------------------------\n","minimax_agent_dept_6 wins: 96.0\n","random_agent wins: 0.0\n","ties:  4.0\n","minimax_agent_dept_6 elo:  1649.9657732304004\n","random_agent elo:  988.5981684649306\n","-----------------------------------------\n","minimax_agent_dept_6 wins: 0.0\n","minimax_agent_dept_3 wins: 0.0\n","ties:  100.0\n","minimax_agent_dept_6 elo:  1452.3057483515097\n","minimax_agent_dept_3 elo:  1450.9834631260055\n","-----------------------------------------\n","minimax_agent_dept_6 wins: 0.0\n","minimax_agent_dept_9 wins: 100.0\n","ties:  0.0\n","minimax_agent_dept_6 elo:  1202.4913507513936\n","minimax_agent_dept_9 elo:  1736.585404845979\n","-----------------------------------------\n","minimax_agent_dept_6 wins: 80.0\n","mcts_agent_10 wins: 0.0\n","ties:  20.0\n","minimax_agent_dept_6 elo:  1229.2902919510877\n","mcts_agent_10 elo:  895.8511296341039\n","-----------------------------------------\n","minimax_agent_dept_6 wins: 64.0\n","mcts_agent_100 wins: 0.0\n","ties:  36.0\n","minimax_agent_dept_6 elo:  1253.0626290533198\n","mcts_agent_100 elo:  985.3527020559706\n","-----------------------------------------\n","minimax_agent_dept_6 wins: 88.0\n","mcts_agent_1000 wins: 0.0\n","ties:  12.0\n","minimax_agent_dept_6 elo:  1385.9793472720125\n","mcts_agent_1000 elo:  956.6497846010017\n","-----------------------------------------\n","minimax_agent_dept_9 wins: 92.0\n","random_agent wins: 1.0\n","ties:  7.000000000000001\n","minimax_agent_dept_9 elo:  1682.4309732305599\n","random_agent elo:  1042.7526000803493\n","-----------------------------------------\n","minimax_agent_dept_9 wins: 0.0\n","minimax_agent_dept_3 wins: 0.0\n","ties:  100.0\n","minimax_agent_dept_9 elo:  1567.0406436183926\n","minimax_agent_dept_3 elo:  1566.3737927381728\n","-----------------------------------------\n","minimax_agent_dept_9 wins: 100.0\n","minimax_agent_dept_6 wins: 0.0\n","ties:  0.0\n","minimax_agent_dept_9 elo:  1752.3533782933062\n","minimax_agent_dept_6 elo:  1200.666612597099\n","-----------------------------------------\n","minimax_agent_dept_9 wins: 67.0\n","mcts_agent_10 wins: 0.0\n","ties:  33.0\n","minimax_agent_dept_9 elo:  1510.9098127966565\n","mcts_agent_10 elo:  1137.294695130754\n","-----------------------------------------\n","minimax_agent_dept_9 wins: 70.0\n","mcts_agent_100 wins: 2.0\n","ties:  28.000000000000004\n","minimax_agent_dept_9 elo:  1408.6127758968996\n","mcts_agent_100 elo:  1087.6497389557273\n","-----------------------------------------\n","minimax_agent_dept_9 wins: 71.0\n","mcts_agent_1000 wins: 2.0\n","ties:  27.0\n","minimax_agent_dept_9 elo:  1359.6279422239281\n","mcts_agent_1000 elo:  1005.634618273973\n","-----------------------------------------\n","mcts_agent_10 wins: 54.0\n","random_agent wins: 21.0\n","ties:  25.0\n","mcts_agent_10 elo:  1186.3128072611428\n","random_agent elo:  993.7344879499609\n","-----------------------------------------\n","mcts_agent_10 wins: 0.0\n","minimax_agent_dept_3 wins: 97.0\n","ties:  3.0\n","mcts_agent_10 elo:  1100.3128562805894\n","minimax_agent_dept_3 elo:  1652.3737437187262\n","-----------------------------------------\n","mcts_agent_10 wins: 1.0\n","minimax_agent_dept_6 wins: 99.0\n","ties:  0.0\n","mcts_agent_10 elo:  888.529938611736\n","minimax_agent_dept_6 elo:  1412.4495302659514\n","-----------------------------------------\n","mcts_agent_10 wins: 0.0\n","minimax_agent_dept_9 wins: 100.0\n","ties:  0.0\n","mcts_agent_10 elo:  809.7910410118271\n","minimax_agent_dept_9 elo:  1438.3668398238362\n","-----------------------------------------\n","mcts_agent_10 wins: 32.0\n","mcts_agent_100 wins: 37.0\n","ties:  31.0\n","mcts_agent_10 elo:  949.7066866293534\n","mcts_agent_100 elo:  947.7340933382023\n","-----------------------------------------\n","mcts_agent_10 wins: 28.999999999999996\n","mcts_agent_1000 wins: 43.0\n","ties:  28.000000000000004\n","mcts_agent_10 elo:  979.7151535709575\n","mcts_agent_1000 elo:  975.6261513323691\n","-----------------------------------------\n","mcts_agent_100 wins: 56.99999999999999\n","random_agent wins: 20.0\n","ties:  23.0\n","mcts_agent_100 elo:  1034.9616943211488\n","random_agent elo:  906.5068869670148\n","-----------------------------------------\n","mcts_agent_100 wins: 0.0\n","minimax_agent_dept_3 wins: 95.0\n","ties:  5.0\n","mcts_agent_100 elo:  1033.6731826456596\n","minimax_agent_dept_3 elo:  1653.6622553942154\n","-----------------------------------------\n","mcts_agent_100 wins: 1.0\n","minimax_agent_dept_6 wins: 93.0\n","ties:  6.0\n","mcts_agent_100 elo:  970.4929013047032\n","minimax_agent_dept_6 elo:  1475.6298116069077\n","-----------------------------------------\n","mcts_agent_100 wins: 0.0\n","minimax_agent_dept_9 wins: 98.0\n","ties:  2.0\n","mcts_agent_100 elo:  904.6469817772844\n","minimax_agent_dept_9 elo:  1504.2127593512557\n","-----------------------------------------\n","mcts_agent_100 wins: 33.0\n","mcts_agent_10 wins: 38.0\n","ties:  28.999999999999996\n","mcts_agent_100 elo:  894.4629420653068\n","mcts_agent_10 elo:  989.899193282935\n","-----------------------------------------\n","mcts_agent_100 wins: 27.0\n","mcts_agent_1000 wins: 40.0\n","ties:  33.0\n","mcts_agent_100 elo:  884.7412249480984\n","mcts_agent_1000 elo:  985.3478684495775\n","-----------------------------------------\n","mcts_agent_1000 wins: 67.0\n","random_agent wins: 14.000000000000002\n","ties:  19.0\n","mcts_agent_1000 elo:  1033.5502277883793\n","random_agent elo:  858.3045276282141\n","-----------------------------------------\n","mcts_agent_1000 wins: 0.0\n","minimax_agent_dept_3 wins: 98.0\n","ties:  2.0\n","mcts_agent_1000 elo:  1005.2968460670585\n","minimax_agent_dept_3 elo:  1681.9156371155366\n","-----------------------------------------\n","mcts_agent_1000 wins: 0.0\n","minimax_agent_dept_6 wins: 92.0\n","ties:  8.0\n","mcts_agent_1000 elo:  977.247278045764\n","minimax_agent_dept_6 elo:  1503.6793796282027\n","-----------------------------------------\n","mcts_agent_1000 wins: 0.0\n","minimax_agent_dept_9 wins: 96.0\n","ties:  4.0\n","mcts_agent_1000 elo:  944.4188458025728\n","minimax_agent_dept_9 elo:  1537.041191594447\n","-----------------------------------------\n","mcts_agent_1000 wins: 38.0\n","mcts_agent_10 wins: 42.0\n","ties:  20.0\n","mcts_agent_1000 elo:  947.5341627352651\n","mcts_agent_10 elo:  986.7838763502426\n","-----------------------------------------\n","mcts_agent_1000 wins: 28.000000000000004\n","mcts_agent_100 wins: 49.0\n","ties:  23.0\n","mcts_agent_1000 elo:  868.8018662698848\n","mcts_agent_100 elo:  963.4735214134787\n","-----------------------------------------\n","random_agent\n","858.3045276282141\n","minimax_agent_dept_3\n","1681.9156371155366\n","minimax_agent_dept_6\n","1503.6793796282027\n","minimax_agent_dept_9\n","1537.041191594447\n","mcts_agent_10\n","986.7838763502426\n","mcts_agent_100\n","963.4735214134787\n","mcts_agent_1000\n","868.8018662698848\n"]}],"source":["train_examples_policy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"atjwSEOEBYLd"},"outputs":[],"source":["# random_agent\n","# 858.3045276282141\n","# minimax_agent_dept_3\n","# 1681.9156371155366\n","# minimax_agent_dept_6\n","# 1503.6793796282027\n","# minimax_agent_dept_9\n","# 1537.041191594447\n","# mcts_agent_10\n","# 986.7838763502426\n","# mcts_agent_100\n","# 963.4735214134787\n","# mcts_agent_1000\n","# 868.8018662698848"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y8AiC3fUcmIG"},"outputs":[],"source":["df"]},{"cell_type":"markdown","metadata":{"id":"ihJcMn0PE2v2"},"source":["### Policy Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nI9Vu4jtE2v2"},"outputs":[],"source":["# def exceute_episode(self):\n","\n","#         train_examples = []\n","#         current_player = 1\n","#         state = self.game.get_init_board()\n","\n","#         while True:\n","#             canonical_board = self.game.get_canonical_board(state, current_player)\n","\n","#             self.mcts = MCTS(self.game, self.model, self.args)\n","#             root = self.mcts.run(self.model, canonical_board, to_play=1)\n","\n","#             action_probs = [0 for _ in range(self.game.get_action_size())]\n","#             for k, v in root.children.items():\n","#                 action_probs[k] = v.visit_count\n","\n","#             action_probs = action_probs / np.sum(action_probs)\n","#             train_examples.append((canonical_board, current_player, action_probs))\n","\n","#             action = root.select_action(temperature=0)\n","#             state, current_player = self.game.get_next_state(state, current_player, action)\n","#             reward = self.game.get_reward_for_player(state, current_player)\n","\n","#             if reward is not None:\n","#                 ret = []\n","#                 for hist_state, hist_current_player, hist_action_probs in train_examples:\n","#                     # [Board, currentPlayer, actionProbabilities, Reward]\n","#                     ret.append((hist_state, hist_action_probs, reward * ((-1) ** (hist_current_player != current_player))))\n","\n","#                 return ret"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N9KGjmbZEXQ2"},"outputs":[],"source":["def format_data(state, action):\n","    x = []\n","    y = action\n","    x.extend(state['board'])\n","    x.append(state['current_player'])\n","    x.append(state['player_territory'][1])\n","\n","    return (x,y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aq2ohI2FBYLf"},"outputs":[],"source":["def exceute_episode(state,  agent_1, agent_2,show=False):\n","    state = copy.deepcopy(state)\n","    train_examples_policy = []\n","    train_examples_value = []\n","    reward = [0,0,0]\n","    path = []\n","\n","    while True:\n","        action = generate_action(state, agent_1, agent_2)\n","        train_examples_policy.append(format_data(state, action))\n","\n","        if is_illegal_move(state, action):\n","            continue\n","\n","        state, new_reward = play(state, action)\n","        reward = assign_reward(reward, new_reward)\n","        if show:\n","            print_game_play(state, reward, action)\n","\n","        if end_game(state):\n","            break\n","        path.append(action)\n","\n","    # print(train_examples_policy)\n","\n","    for hist_state, _ in train_examples_policy:\n","        # [Board, currentPlayer, actionProbabilities, Reward]\n","        train_examples_value.append((hist_state, reward[hist_state[12]]))\n","\n","    return  (train_examples_policy, train_examples_value)"]},{"cell_type":"code","source":["def match_up(origin_state,number_of_games, player_1, player_2 ):\n","   results = {\n","         'player_1_name': player_1['name'],\n","         'player_2_name': player_2['name'],\n","         'player_1_wins': 0,\n","         'player_2_wins': 0,\n","         'ties': 0,\n","         'player_1_elo': player_1['elo'],\n","         'player_2_elo': player_2['elo'],\n","      }\n","\n","   for i in range(number_of_games):\n","      reward, path = simulate_game(origin_state, player_1, player_2)\n","      # paths.append([reward,path])\n","\n","      if reward[0] > reward[1]:\n","         results['player_1_wins'] += 1\n","         elo_0, elo_1 = calculate_elo(player_1['elo'],player_2['elo'],1)\n","      elif reward[0] < reward[1]:\n","         results['player_2_wins'] += 1\n","         elo_0, elo_1 = calculate_elo(player_1['elo'],player_2['elo'],0)\n","      elif reward[0] == reward[1]:\n","         results['ties'] += 1\n","         elo_0, elo_1 = calculate_elo(player_1['elo'],player_2['elo'],0.5)\n","\n","      player_1['elo'],player_2['elo'] = (elo_0, elo_1)\n","\n","   results['player_1_elo'] = player_1['elo']\n","   results['player_2_elo'] = player_2['elo']\n","   results['player_1_wins'] = results['player_1_wins']/number_of_games * 100\n","   results['player_2_wins'] = results['player_2_wins']/number_of_games * 100\n","   results['ties'] = results['ties']/number_of_games * 100\n","\n","   return results"],"metadata":{"id":"jk0ZUFlhGDj5"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LOoRSKjUBYLg"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","# from tensorflow.keras.models import\n","from tensorflow.keras.layers import Dense"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m1asEKk6Bfjr"},"outputs":[],"source":["X = np.array([[d['pit 0'], d['pit 1'], d['pit 2'], d['pit 3'], d['pit 4'], d['pit 5'],\n","               d['pit 6'], d['pit 7'], d['pit 8'], d['pit 9'], d['pit 10'], d['pit 11'],\n","               d['current_player'], d['player_territory']] for d in data])\n","\n","y = np.array([d['action'] for d in data])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n80UlvfQBi9Y"},"outputs":[],"source":["# Convert target (y) to categorical (for classification)\n","num_actions = 12  # Assuming 12 possible actions (0 to 11)\n","y = tf.keras.utils.to_categorical(y, num_classes=num_actions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t2kzKf82BlI1"},"outputs":[],"source":["# Build the model\n","model = Sequential([\n","    Dense(64, input_dim=X.shape[1], activation='relu'),  # First hidden layer\n","    Dense(32, activation='relu'),  # Second hidden layer\n","    Dense(num_actions, activation='softmax')  # Output layer for 12 possible actions\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EUSz83xPBokf"},"outputs":[],"source":["# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model (Assume more training data is available)\n","# For example purposes, we'll use the same sample data multiple times\n","model.fit(X, y, epochs=10, batch_size=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ieGnR40Bryq"},"outputs":[],"source":["# Predict action for a new data point\n","new_data = np.array([[4, 0, 4, 0, 4, 0, 0, 1, 2, 0, 10, 4, 0, 6]])\n","prediction = model.predict(new_data)\n","print(prediction)\n","predicted_action = np.argmax(prediction)\n","print(f\"Predicted action: {predicted_action}\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}