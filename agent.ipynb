{"cells":[{"cell_type":"markdown","metadata":{"id":"XFCWflMKcmH-"},"source":["# New Game Play"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"3nc7NVffcmIA"},"outputs":[],"source":["import copy\n","import random\n","from json.encoder import INFINITY\n","import pandas as pd\n","import math\n","from ayo_game import play, is_illegal_move, assign_reward, print_game_play, end_game\n","from agents import random_agent as ra\n","from agents import minimax_agent as ma\n","seed = 37\n","\n","randint = random.randint\n","random.seed(seed)"]},{"cell_type":"markdown","metadata":{"id":"ZmUR1hNecmIE"},"source":["## Agents"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["random_agent = ra.agent\n","minimax_agent = ma.agent\n","# minimax_agent = ma.agent"]},{"cell_type":"markdown","metadata":{"id":"2mzdjbUkcmIE"},"source":["## Simulation"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def generate_action(state,agent_1, agent_2):\n","   if state['current_player'] == 0:\n","      func = agent_1['func']\n","      arg = agent_1['arg']\n","      return func(state, arg)\n","\n","   if state['current_player'] == 1:\n","      func = agent_2['func']\n","      arg = agent_2['arg']\n","      return func(state, arg)\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"CaGotfb5cmIE"},"outputs":[],"source":["def simulate_game(state,  agent_1, agent_2,show=False):\n","    state = copy.deepcopy(state)\n","    reward = [0,0,0]\n","    path = []\n","\n","    while True:\n","        action = generate_action(state, agent_1, agent_2)\n","\n","        if is_illegal_move(state, action):\n","            continue\n","\n","        state, new_reward = play(state, action)\n","        reward = assign_reward(reward, new_reward)\n","        if show:\n","            print_game_play(state, reward, action)\n","\n","        if end_game(state):\n","            break\n","        path.append(action)\n","\n","    return  (reward, path)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"qsc1-WGVcmIE"},"outputs":[],"source":["def get_valid_actions_mct(state):\n","    board = state['board']\n","    territory = state['player_territory'][1]\n","    current_player = state['current_player']\n","    valid_actions = []\n","    for i,a in enumerate(board):\n","        if current_player == 0 and i < territory and a != 0:\n","                    valid_actions.append(i)\n","        if current_player == 1 and i >= territory and a != 0:\n","                    valid_actions.append(i)\n","    return valid_actions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n8cr8O-SE2vx"},"outputs":[],"source":["def format_state(state, action):\n","    board = state['board']\n","    data = []\n","    for i,pit in enumerate(board):\n","        num = str(i)\n","        data[f'pit {num}'] = pit\n","    data['current_player'] = state['current_player']\n","    data['player_territory'] = state['player_territory'][1]\n","    data['action'] = action\n","    return [data]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pbl5uJ1IE2vx"},"outputs":[],"source":["state = {\n","   'board' :[4,4,4,4,4,4,4,4,4,4,4,4],\n","   'current_player': 0,\n","   'player_territory': (0,6)\n","}\n","\n","action = 0\n","format_state(state, action)"]},{"cell_type":"markdown","metadata":{"id":"9UsY5drsE2vy"},"source":["## Elo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4AmmIfFiE2v1"},"outputs":[],"source":["def update_score(actual_result, expected_result):\n","    k = 20\n","    return k * (actual_result-expected_result)\n","\n","def expected_score(elo_1,elo_2):\n","    diff = elo_2 - elo_1\n","    a = diff/400\n","    d = 1 + 10**a\n","    return 1/d\n","\n","def calculate_elo(elo_0,elo_1,result):\n","    expected_result = expected_score(elo_0,elo_1)\n","    change = update_score(result, expected_result)\n","\n","    new_elo_0 = elo_0 + change\n","    new_elo_1 = elo_1 - change\n","\n","    return (new_elo_0, new_elo_1)"]},{"cell_type":"markdown","metadata":{"id":"-HN0_FCrpJG8"},"source":["## Deep Reinforcement Learning (DRL)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mmGfGXocpONb"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","# from tensorflow.keras.models import\n","from tensorflow.keras.layers import Dense"]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":408,"status":"ok","timestamp":1729342276504,"user":{"displayName":"Nwachukwu Ujubuonu","userId":"09987348326723992864"},"user_tz":-60},"id":"YyjeyPOb1EX0"},"outputs":[],"source":["def format_state(state):\n","    x = []\n","    x.extend(state['board'])\n","    x.append(state['current_player'])\n","    x.append(state['player_territory'][1])\n","\n","    return x"]},{"cell_type":"code","execution_count":80,"metadata":{"executionInfo":{"elapsed":443,"status":"ok","timestamp":1729343396944,"user":{"displayName":"Nwachukwu Ujubuonu","userId":"09987348326723992864"},"user_tz":-60},"id":"xbCLVYhHpUYN"},"outputs":[],"source":["class ayo_drl_model:\n","    def __init__(self, X_shape):\n","      self.num_actions=12\n","      self.model = Sequential([\n","          Dense(64, input_dim=X_shape, activation='relu'),  # First hidden layer\n","          Dense(32, activation='relu'),  # Second hidden layer\n","          Dense(self.num_actions, activation='softmax')  # Output layer for 12 possible actions\n","      ])\n","\n","            # Compile the model\n","      self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    def train_policy(self, training_examples):\n","      X = np.array([d[0] for d in training_examples])\n","      y = np.array([d[1] for d in training_examples])\n","\n","      # Convert target (y) to categorical (for classification)\n","      num_actions = self.num_actions # Assuming 12 possible actions (0 to 11)\n","      y = tf.keras.utils.to_categorical(y, num_classes=num_actions)\n","\n","      # Train the model (Assume more training data is available)\n","      # For example purposes, we'll use the same sample data multiple times\n","      self.model.fit(X, y, epochs=10, batch_size=1)\n","\n","\n","    def predict(self,data):\n","      # Predict action for a new data point\n","      formatted_data = np.array([format_state(data)])\n","      prediction = self.model.predict(formatted_data)\n","      return prediction\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gnXDAU1K6ePw"},"outputs":[],"source":["def ayo_drl_agent(state,arg):\n","    model = arg['model']\n","\n","    action = model.predict(state)\n","    print(prediction)\n","    predicted_action = np.argmax(prediction)\n","    print(f\"Predicted action: {predicted_action}\")\n","    return np.argmax(action)"]},{"cell_type":"markdown","metadata":{"id":"edZ9zYTiE2v1"},"source":["## Test"]},{"cell_type":"markdown","metadata":{"id":"zMrCCCh3E2v1"},"source":["### Agents Details"]},{"cell_type":"code","execution_count":81,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":657,"status":"error","timestamp":1729343769845,"user":{"displayName":"Nwachukwu Ujubuonu","userId":"09987348326723992864"},"user_tz":-60},"id":"PDWmrBWbE2v2","outputId":"dd035f0a-207a-4f46-9831-404f70a04f68"},"outputs":[{"ename":"NameError","evalue":"name 'random_agent' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-81-ef80f852c8b3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m random_agent_details = {\n\u001b[0;32m----> 2\u001b[0;31m    \u001b[0;34m'func'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrandom_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m    \u001b[0;34m'arg'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m    \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'random_agent'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m    \u001b[0;34m'elo'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'random_agent' is not defined"]}],"source":["random_agent_details = {\n","   'func': random_agent,\n","   'arg': {},\n","   'name': 'random_agent',\n","   'elo': 1200\n","}\n","\n","mcts_agent_details_10 = {\n","   'func': mcts_agent,\n","   'arg': {\n","      'max_iterations': 10,\n","   },\n","   'name': 'mcts_agent_10',\n","   'elo': 1200\n","}\n","\n","mcts_agent_details_100 = {\n","   'func': mcts_agent,\n","   'arg': {\n","      'max_iterations': 100,\n","   },\n","   'name': 'mcts_agent_100',\n","   'elo': 1200\n","}\n","\n","mcts_agent_details_1000 = {\n","   'func': mcts_agent,\n","   'arg': {\n","      'max_iterations': 1000,\n","   },\n","   'name': 'mcts_agent_1000',\n","   'elo': 1200\n","}\n","\n","minimax_agent_details_3 = {\n","   'func': minimax_agent,\n","   'arg': {\n","      'max_dept': 3,\n","   },\n","   'name': 'minimax_agent_dept_3',\n","   'elo': 1200\n","}\n","\n","minimax_agent_details_6 = {\n","   'func': minimax_agent,\n","   'arg': {\n","      'max_dept': 6,\n","   },\n","   'name': 'minimax_agent_dept_6',\n","   'elo': 1200\n","}\n","\n","minimax_agent_details_9 = {\n","   'func': minimax_agent,\n","   'arg': {\n","      'max_dept': 9,\n","   },\n","   'name': 'minimax_agent_dept_9',\n","   'elo': 1200\n","}\n","\n","dlr_agent_details = {\n","   'func': ayo_drl_agent,\n","   'arg': {\n","      'model': ayo_drl_model,\n","   },\n","   'name': 'dlr_agent_details',\n","   'elo': 1200\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M7OBWaBXE2v2"},"outputs":[],"source":["def match_up(origin_state,number_of_games, player_1, player_2 ):\n","   results = {\n","         'player_1_name': player_1['name'],\n","         'player_2_name': player_2['name'],\n","         'player_1_wins': 0,\n","         'player_2_wins': 0,\n","         'ties': 0,\n","         'player_1_elo': player_1['elo'],\n","         'player_2_elo': player_2['elo'],\n","      }\n","\n","   for i in range(number_of_games):\n","      reward, path = simulate_game(origin_state, player_1, player_2)\n","      # paths.append([reward,path])\n","\n","      if reward[0] > reward[1]:\n","         results['player_1_wins'] += 1\n","         elo_0, elo_1 = calculate_elo(player_1['elo'],player_2['elo'],1)\n","      elif reward[0] < reward[1]:\n","         results['player_2_wins'] += 1\n","         elo_0, elo_1 = calculate_elo(player_1['elo'],player_2['elo'],0)\n","      elif reward[0] == reward[1]:\n","         results['ties'] += 1\n","         elo_0, elo_1 = calculate_elo(player_1['elo'],player_2['elo'],0.5)\n","\n","      player_1['elo'],player_2['elo'] = (elo_0, elo_1)\n","\n","   results['player_1_elo'] = player_1['elo']\n","   results['player_2_elo'] = player_2['elo']\n","   results['player_1_wins'] = results['player_1_wins']/number_of_games * 100\n","   results['player_2_wins'] = results['player_2_wins']/number_of_games * 100\n","   results['ties'] = results['ties']/number_of_games * 100\n","\n","   return results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16966657,"status":"ok","timestamp":1729296387674,"user":{"displayName":"Nwachukwu Ujubuonu","userId":"09987348326723992864"},"user_tz":-60},"id":"hMHcAEg9cmIG","outputId":"5dfafb0f-45c7-472f-ce3b-e94914ba7b71"},"outputs":[{"name":"stdout","output_type":"stream","text":["random_agent wins: 0.0\n","minimax_agent_dept_3 wins: 98.0\n","ties:  2.0\n","random_agent elo:  940.7576240420559\n","minimax_agent_dept_3 elo:  1459.2423759579453\n","-----------------------------------------\n","random_agent wins: 0.0\n","minimax_agent_dept_6 wins: 98.0\n","ties:  2.0\n","random_agent elo:  803.6745837740348\n","minimax_agent_dept_6 elo:  1337.0830402680217\n","-----------------------------------------\n","random_agent wins: 0.0\n","minimax_agent_dept_9 wins: 100.0\n","ties:  0.0\n","random_agent elo:  701.4395706632646\n","minimax_agent_dept_9 elo:  1302.23501311077\n","-----------------------------------------\n","random_agent wins: 15.0\n","mcts_agent_10 wins: 65.0\n","ties:  20.0\n","random_agent elo:  825.023880098059\n","mcts_agent_10 elo:  1076.4156905652064\n","-----------------------------------------\n","random_agent wins: 16.0\n","mcts_agent_100 wins: 62.0\n","ties:  22.0\n","random_agent elo:  907.5428721280138\n","mcts_agent_100 elo:  1117.4810079700449\n","-----------------------------------------\n","random_agent wins: 19.0\n","mcts_agent_1000 wins: 56.99999999999999\n","ties:  24.0\n","random_agent elo:  961.5814682506651\n","mcts_agent_1000 elo:  1145.9614038773493\n","-----------------------------------------\n","minimax_agent_dept_3 wins: 87.0\n","random_agent wins: 5.0\n","ties:  8.0\n","minimax_agent_dept_3 elo:  1425.4850264393715\n","random_agent elo:  995.3388177692391\n","-----------------------------------------\n","minimax_agent_dept_3 wins: 0.0\n","minimax_agent_dept_6 wins: 100.0\n","ties:  0.0\n","minimax_agent_dept_3 elo:  1119.3429427813016\n","minimax_agent_dept_6 elo:  1643.2251239260916\n","-----------------------------------------\n","minimax_agent_dept_3 wins: 0.0\n","minimax_agent_dept_9 wins: 100.0\n","ties:  0.0\n","minimax_agent_dept_3 elo:  934.8069486462083\n","minimax_agent_dept_9 elo:  1486.7710072458628\n","-----------------------------------------\n","minimax_agent_dept_3 wins: 45.0\n","mcts_agent_10 wins: 6.0\n","ties:  49.0\n","minimax_agent_dept_3 elo:  1088.5725683776172\n","mcts_agent_10 elo:  922.6500708337988\n","-----------------------------------------\n","minimax_agent_dept_3 wins: 56.00000000000001\n","mcts_agent_100 wins: 13.0\n","ties:  31.0\n","minimax_agent_dept_3 elo:  1196.9285371894596\n","mcts_agent_100 elo:  1009.125039158202\n","-----------------------------------------\n","minimax_agent_dept_3 wins: 46.0\n","mcts_agent_1000 wins: 2.0\n","ties:  52.0\n","minimax_agent_dept_3 elo:  1253.3234382471148\n","mcts_agent_1000 elo:  1089.566502819694\n","-----------------------------------------\n","minimax_agent_dept_6 wins: 96.0\n","random_agent wins: 0.0\n","ties:  4.0\n","minimax_agent_dept_6 elo:  1649.9657732304004\n","random_agent elo:  988.5981684649306\n","-----------------------------------------\n","minimax_agent_dept_6 wins: 0.0\n","minimax_agent_dept_3 wins: 0.0\n","ties:  100.0\n","minimax_agent_dept_6 elo:  1452.3057483515097\n","minimax_agent_dept_3 elo:  1450.9834631260055\n","-----------------------------------------\n","minimax_agent_dept_6 wins: 0.0\n","minimax_agent_dept_9 wins: 100.0\n","ties:  0.0\n","minimax_agent_dept_6 elo:  1202.4913507513936\n","minimax_agent_dept_9 elo:  1736.585404845979\n","-----------------------------------------\n","minimax_agent_dept_6 wins: 80.0\n","mcts_agent_10 wins: 0.0\n","ties:  20.0\n","minimax_agent_dept_6 elo:  1229.2902919510877\n","mcts_agent_10 elo:  895.8511296341039\n","-----------------------------------------\n","minimax_agent_dept_6 wins: 64.0\n","mcts_agent_100 wins: 0.0\n","ties:  36.0\n","minimax_agent_dept_6 elo:  1253.0626290533198\n","mcts_agent_100 elo:  985.3527020559706\n","-----------------------------------------\n","minimax_agent_dept_6 wins: 88.0\n","mcts_agent_1000 wins: 0.0\n","ties:  12.0\n","minimax_agent_dept_6 elo:  1385.9793472720125\n","mcts_agent_1000 elo:  956.6497846010017\n","-----------------------------------------\n","minimax_agent_dept_9 wins: 92.0\n","random_agent wins: 1.0\n","ties:  7.000000000000001\n","minimax_agent_dept_9 elo:  1682.4309732305599\n","random_agent elo:  1042.7526000803493\n","-----------------------------------------\n","minimax_agent_dept_9 wins: 0.0\n","minimax_agent_dept_3 wins: 0.0\n","ties:  100.0\n","minimax_agent_dept_9 elo:  1567.0406436183926\n","minimax_agent_dept_3 elo:  1566.3737927381728\n","-----------------------------------------\n","minimax_agent_dept_9 wins: 100.0\n","minimax_agent_dept_6 wins: 0.0\n","ties:  0.0\n","minimax_agent_dept_9 elo:  1752.3533782933062\n","minimax_agent_dept_6 elo:  1200.666612597099\n","-----------------------------------------\n","minimax_agent_dept_9 wins: 67.0\n","mcts_agent_10 wins: 0.0\n","ties:  33.0\n","minimax_agent_dept_9 elo:  1510.9098127966565\n","mcts_agent_10 elo:  1137.294695130754\n","-----------------------------------------\n","minimax_agent_dept_9 wins: 70.0\n","mcts_agent_100 wins: 2.0\n","ties:  28.000000000000004\n","minimax_agent_dept_9 elo:  1408.6127758968996\n","mcts_agent_100 elo:  1087.6497389557273\n","-----------------------------------------\n","minimax_agent_dept_9 wins: 71.0\n","mcts_agent_1000 wins: 2.0\n","ties:  27.0\n","minimax_agent_dept_9 elo:  1359.6279422239281\n","mcts_agent_1000 elo:  1005.634618273973\n","-----------------------------------------\n","mcts_agent_10 wins: 54.0\n","random_agent wins: 21.0\n","ties:  25.0\n","mcts_agent_10 elo:  1186.3128072611428\n","random_agent elo:  993.7344879499609\n","-----------------------------------------\n","mcts_agent_10 wins: 0.0\n","minimax_agent_dept_3 wins: 97.0\n","ties:  3.0\n","mcts_agent_10 elo:  1100.3128562805894\n","minimax_agent_dept_3 elo:  1652.3737437187262\n","-----------------------------------------\n","mcts_agent_10 wins: 1.0\n","minimax_agent_dept_6 wins: 99.0\n","ties:  0.0\n","mcts_agent_10 elo:  888.529938611736\n","minimax_agent_dept_6 elo:  1412.4495302659514\n","-----------------------------------------\n","mcts_agent_10 wins: 0.0\n","minimax_agent_dept_9 wins: 100.0\n","ties:  0.0\n","mcts_agent_10 elo:  809.7910410118271\n","minimax_agent_dept_9 elo:  1438.3668398238362\n","-----------------------------------------\n","mcts_agent_10 wins: 32.0\n","mcts_agent_100 wins: 37.0\n","ties:  31.0\n","mcts_agent_10 elo:  949.7066866293534\n","mcts_agent_100 elo:  947.7340933382023\n","-----------------------------------------\n","mcts_agent_10 wins: 28.999999999999996\n","mcts_agent_1000 wins: 43.0\n","ties:  28.000000000000004\n","mcts_agent_10 elo:  979.7151535709575\n","mcts_agent_1000 elo:  975.6261513323691\n","-----------------------------------------\n","mcts_agent_100 wins: 56.99999999999999\n","random_agent wins: 20.0\n","ties:  23.0\n","mcts_agent_100 elo:  1034.9616943211488\n","random_agent elo:  906.5068869670148\n","-----------------------------------------\n","mcts_agent_100 wins: 0.0\n","minimax_agent_dept_3 wins: 95.0\n","ties:  5.0\n","mcts_agent_100 elo:  1033.6731826456596\n","minimax_agent_dept_3 elo:  1653.6622553942154\n","-----------------------------------------\n","mcts_agent_100 wins: 1.0\n","minimax_agent_dept_6 wins: 93.0\n","ties:  6.0\n","mcts_agent_100 elo:  970.4929013047032\n","minimax_agent_dept_6 elo:  1475.6298116069077\n","-----------------------------------------\n","mcts_agent_100 wins: 0.0\n","minimax_agent_dept_9 wins: 98.0\n","ties:  2.0\n","mcts_agent_100 elo:  904.6469817772844\n","minimax_agent_dept_9 elo:  1504.2127593512557\n","-----------------------------------------\n","mcts_agent_100 wins: 33.0\n","mcts_agent_10 wins: 38.0\n","ties:  28.999999999999996\n","mcts_agent_100 elo:  894.4629420653068\n","mcts_agent_10 elo:  989.899193282935\n","-----------------------------------------\n","mcts_agent_100 wins: 27.0\n","mcts_agent_1000 wins: 40.0\n","ties:  33.0\n","mcts_agent_100 elo:  884.7412249480984\n","mcts_agent_1000 elo:  985.3478684495775\n","-----------------------------------------\n","mcts_agent_1000 wins: 67.0\n","random_agent wins: 14.000000000000002\n","ties:  19.0\n","mcts_agent_1000 elo:  1033.5502277883793\n","random_agent elo:  858.3045276282141\n","-----------------------------------------\n","mcts_agent_1000 wins: 0.0\n","minimax_agent_dept_3 wins: 98.0\n","ties:  2.0\n","mcts_agent_1000 elo:  1005.2968460670585\n","minimax_agent_dept_3 elo:  1681.9156371155366\n","-----------------------------------------\n","mcts_agent_1000 wins: 0.0\n","minimax_agent_dept_6 wins: 92.0\n","ties:  8.0\n","mcts_agent_1000 elo:  977.247278045764\n","minimax_agent_dept_6 elo:  1503.6793796282027\n","-----------------------------------------\n","mcts_agent_1000 wins: 0.0\n","minimax_agent_dept_9 wins: 96.0\n","ties:  4.0\n","mcts_agent_1000 elo:  944.4188458025728\n","minimax_agent_dept_9 elo:  1537.041191594447\n","-----------------------------------------\n","mcts_agent_1000 wins: 38.0\n","mcts_agent_10 wins: 42.0\n","ties:  20.0\n","mcts_agent_1000 elo:  947.5341627352651\n","mcts_agent_10 elo:  986.7838763502426\n","-----------------------------------------\n","mcts_agent_1000 wins: 28.000000000000004\n","mcts_agent_100 wins: 49.0\n","ties:  23.0\n","mcts_agent_1000 elo:  868.8018662698848\n","mcts_agent_100 elo:  963.4735214134787\n","-----------------------------------------\n","random_agent\n","858.3045276282141\n","minimax_agent_dept_3\n","1681.9156371155366\n","minimax_agent_dept_6\n","1503.6793796282027\n","minimax_agent_dept_9\n","1537.041191594447\n","mcts_agent_10\n","986.7838763502426\n","mcts_agent_100\n","963.4735214134787\n","mcts_agent_1000\n","868.8018662698848\n"]}],"source":["train_examples_policy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"atjwSEOEBYLd"},"outputs":[],"source":["# random_agent\n","# 858.3045276282141\n","# minimax_agent_dept_3\n","# 1681.9156371155366\n","# minimax_agent_dept_6\n","# 1503.6793796282027\n","# minimax_agent_dept_9\n","# 1537.041191594447\n","# mcts_agent_10\n","# 986.7838763502426\n","# mcts_agent_100\n","# 963.4735214134787\n","# mcts_agent_1000\n","# 868.8018662698848"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y8AiC3fUcmIG"},"outputs":[],"source":["df"]},{"cell_type":"markdown","metadata":{"id":"ihJcMn0PE2v2"},"source":["### Policy Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nI9Vu4jtE2v2"},"outputs":[],"source":["# def exceute_episode(self):\n","\n","#         train_examples = []\n","#         current_player = 1\n","#         state = self.game.get_init_board()\n","\n","#         while True:\n","#             canonical_board = self.game.get_canonical_board(state, current_player)\n","\n","#             self.mcts = MCTS(self.game, self.model, self.args)\n","#             root = self.mcts.run(self.model, canonical_board, to_play=1)\n","\n","#             action_probs = [0 for _ in range(self.game.get_action_size())]\n","#             for k, v in root.children.items():\n","#                 action_probs[k] = v.visit_count\n","\n","#             action_probs = action_probs / np.sum(action_probs)\n","#             train_examples.append((canonical_board, current_player, action_probs))\n","\n","#             action = root.select_action(temperature=0)\n","#             state, current_player = self.game.get_next_state(state, current_player, action)\n","#             reward = self.game.get_reward_for_player(state, current_player)\n","\n","#             if reward is not None:\n","#                 ret = []\n","#                 for hist_state, hist_current_player, hist_action_probs in train_examples:\n","#                     # [Board, currentPlayer, actionProbabilities, Reward]\n","#                     ret.append((hist_state, hist_action_probs, reward * ((-1) ** (hist_current_player != current_player))))\n","\n","#                 return ret"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N9KGjmbZEXQ2"},"outputs":[],"source":["def format_data(state, action):\n","    x = []\n","    y = action\n","    x.extend(state['board'])\n","    x.append(state['current_player'])\n","    x.append(state['player_territory'][1])\n","\n","    return (x,y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aq2ohI2FBYLf"},"outputs":[],"source":["def exceute_episode(state,  agent_1, agent_2,show=False):\n","    state = copy.deepcopy(state)\n","    train_examples_policy = []\n","    train_examples_value = []\n","    reward = [0,0,0]\n","    path = []\n","\n","    while True:\n","        action = generate_action(state, agent_1, agent_2)\n","        train_examples_policy.append(format_data(state, action))\n","\n","        if is_illegal_move(state, action):\n","            continue\n","\n","        state, new_reward = play(state, action)\n","        reward = assign_reward(reward, new_reward)\n","        if show:\n","            print_game_play(state, reward, action)\n","\n","        if end_game(state):\n","            break\n","        path.append(action)\n","\n","    # print(train_examples_policy)\n","\n","    for hist_state, _ in train_examples_policy:\n","        # [Board, currentPlayer, actionProbabilities, Reward]\n","        train_examples_value.append((hist_state, reward[hist_state[12]]))\n","\n","    return  (train_examples_policy, train_examples_value)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jk0ZUFlhGDj5"},"outputs":[],"source":["def match_up(origin_state,number_of_games, player_1, player_2 ):\n","   results = {\n","         'player_1_name': player_1['name'],\n","         'player_2_name': player_2['name'],\n","         'player_1_wins': 0,\n","         'player_2_wins': 0,\n","         'ties': 0,\n","         'player_1_elo': player_1['elo'],\n","         'player_2_elo': player_2['elo'],\n","      }\n","\n","   for i in range(number_of_games):\n","      reward, path = simulate_game(origin_state, player_1, player_2)\n","      # paths.append([reward,path])\n","\n","      if reward[0] > reward[1]:\n","         results['player_1_wins'] += 1\n","         elo_0, elo_1 = calculate_elo(player_1['elo'],player_2['elo'],1)\n","      elif reward[0] < reward[1]:\n","         results['player_2_wins'] += 1\n","         elo_0, elo_1 = calculate_elo(player_1['elo'],player_2['elo'],0)\n","      elif reward[0] == reward[1]:\n","         results['ties'] += 1\n","         elo_0, elo_1 = calculate_elo(player_1['elo'],player_2['elo'],0.5)\n","\n","      player_1['elo'],player_2['elo'] = (elo_0, elo_1)\n","\n","   results['player_1_elo'] = player_1['elo']\n","   results['player_2_elo'] = player_2['elo']\n","   results['player_1_wins'] = results['player_1_wins']/number_of_games * 100\n","   results['player_2_wins'] = results['player_2_wins']/number_of_games * 100\n","   results['ties'] = results['ties']/number_of_games * 100\n","\n","   return results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LOoRSKjUBYLg"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","# from tensorflow.keras.models import\n","from tensorflow.keras.layers import Dense"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m1asEKk6Bfjr"},"outputs":[],"source":["X = np.array([[d['pit 0'], d['pit 1'], d['pit 2'], d['pit 3'], d['pit 4'], d['pit 5'],\n","               d['pit 6'], d['pit 7'], d['pit 8'], d['pit 9'], d['pit 10'], d['pit 11'],\n","               d['current_player'], d['player_territory']] for d in data])\n","\n","y = np.array([d['action'] for d in data])\n","\n","\n","# Convert target (y) to categorical (for classification)\n","num_actions = 12  # Assuming 12 possible actions (0 to 11)\n","y = tf.keras.utils.to_categorical(y, num_classes=num_actions)\n","\n","# Build the model\n","model = Sequential([\n","    Dense(64, input_dim=X.shape[1], activation='relu'),  # First hidden layer\n","    Dense(32, activation='relu'),  # Second hidden layer\n","    Dense(num_actions, activation='softmax')  # Output layer for 12 possible actions\n","])\n","\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model (Assume more training data is available)\n","# For example purposes, we'll use the same sample data multiple times\n","model.fit(X, y, epochs=10, batch_size=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n80UlvfQBi9Y"},"outputs":[],"source":["# Convert target (y) to categorical (for classification)\n","num_actions = 12  # Assuming 12 possible actions (0 to 11)\n","y = tf.keras.utils.to_categorical(y, num_classes=num_actions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t2kzKf82BlI1"},"outputs":[],"source":["# Build the model\n","model = Sequential([\n","    Dense(64, input_dim=X.shape[1], activation='relu'),  # First hidden layer\n","    Dense(32, activation='relu'),  # Second hidden layer\n","    Dense(num_actions, activation='softmax')  # Output layer for 12 possible actions\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EUSz83xPBokf"},"outputs":[],"source":["# Compile the model\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model (Assume more training data is available)\n","# For example purposes, we'll use the same sample data multiple times\n","model.fit(X, y, epochs=10, batch_size=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ieGnR40Bryq"},"outputs":[],"source":["# Predict action for a new data point\n","new_data = np.array([[4, 0, 4, 0, 4, 0, 0, 1, 2, 0, 10, 4, 0, 6]])\n","prediction = model.predict(new_data)\n","print(prediction)\n","predicted_action = np.argmax(prediction)\n","print(f\"Predicted action: {predicted_action}\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}
