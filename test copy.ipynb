{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    " \n",
    "\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "from json.encoder import INFINITY\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "from ayo_game import play, is_illegal_move, assign_reward, print_game_play, end_game\n",
    "from agents import random_agent as ra\n",
    "from agents import minimax_agent as ma\n",
    "from agents import mcts_agent as mctsa\n",
    "seed = 37\n",
    "\n",
    "randint = random.randint\n",
    "random.seed(seed)\n",
    "value_model_file = 'latest_value_model.keras'\n",
    "policy_model_file = 'latest_policy_model.keras'\n",
    "\n",
    "# checkpoint_filepath = 'latest.keras'\n",
    "# checkpoint_filepath = 'multi.keras'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimax_agent = ma.agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_state(state):\n",
    "    x = []\n",
    "    x.extend(state['board'])\n",
    "    x.append(state['current_player'])\n",
    "    x.append(state['player_territory'][1])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(val_1, val_2):\n",
    "    diff = val_1 - val_2\n",
    "\n",
    "    if diff < 0:\n",
    "        return -1\n",
    "    if diff > 0:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_train_test_split(data, test_size=0.2, shuffle=True, random_state=None):\n",
    "    \"\"\"\n",
    "    Custom function to split data into training and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input features (numpy array or list)\n",
    "    - y: Target labels (numpy array or list)\n",
    "    - test_size: Proportion of the data to include in the test split (default 0.2)\n",
    "    - shuffle: Whether to shuffle the data before splitting (default True)\n",
    "    - random_state: Seed for the random number generator (optional, for reproducibility)\n",
    "\n",
    "    Returns:\n",
    "    - X_train, X_test, y_train, y_test: Split training and testing data\n",
    "    \"\"\"\n",
    "    X = np.array([d[0:14] for d in data])\n",
    "    y = np.array([d[14] for d in data])\n",
    "\n",
    "    # Set random seed if provided (to ensure reproducibility)\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    # Get the number of samples\n",
    "    num_samples = len(X)\n",
    "\n",
    "    # Shuffle the data if requested\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(num_samples)\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "\n",
    "    # Compute the split index\n",
    "    split_index = int(num_samples * (1 - test_size))\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = X[:split_index], X[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_duplicates(arr):\n",
    "    # Convert the list of lists to a NumPy array\n",
    "    np_arr = np.array(arr, dtype=object)\n",
    "    \n",
    "    # Create a set to keep track of unique first elements (list of numbers)\n",
    "    seen = set()\n",
    "    unique_arr = []\n",
    "\n",
    "    for item in np_arr:\n",
    "        # Convert the list of numbers (first element) to a tuple so it can be added to a set\n",
    "        num_tuple = tuple(item[0])\n",
    "        if num_tuple not in seen:\n",
    "            unique_arr.append(item)\n",
    "            seen.add(num_tuple)\n",
    "    \n",
    "    return np.array(unique_arr, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data_to_csv(file_name, new_data, y_column=None):\n",
    "    columns = ['Pit_1', 'Pit_2', 'Pit_3','Pit_4', 'Pit_5', 'Pit_6', 'Pit_7', 'Pit_8', 'Pit_9', 'Pit_10', 'Pit_11', 'Pit_12', 'current_player', 'player_territory', y_column]  # Define columns (used only if file doesn't exist)\n",
    "    \"\"\"\n",
    "    Adds new data to a CSV file by first converting the new data and the existing data to a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - file_name: The name of the CSV file (e.g., 'data.csv').\n",
    "    - new_data: A list or tuple representing a new row (or multiple rows) to be added.\n",
    "    - columns: A list of column names for the CSV file (used when the file does not already exist).\n",
    "    \"\"\"\n",
    "    # Check if the CSV file already exists\n",
    "    if os.path.exists(file_name):\n",
    "        # Load existing CSV into a DataFrame\n",
    "        df_existing = pd.read_csv(file_name)\n",
    "    else:\n",
    "        # If the file doesn't exist, create an empty DataFrame with the specified columns\n",
    "        df_existing = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Create a DataFrame from the new data\n",
    "    # Check if new_data is a list of lists (multiple rows) or a single row\n",
    "    if isinstance(new_data[0], (list, tuple)):\n",
    "        df_new = pd.DataFrame(new_data, columns=columns)\n",
    "    else:\n",
    "        df_new = pd.DataFrame([new_data], columns=columns)\n",
    "\n",
    "    # Append the new data to the existing DataFrame\n",
    "    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "\n",
    "    # Write the combined DataFrame back to the CSV file\n",
    "    df_combined.to_csv(file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyModel:\n",
    "    def __init__(self):\n",
    "        self.num_actions=12\n",
    "        self.X_shape=14\n",
    "        self.model = Sequential([\n",
    "            Dense(64, input_dim=self.X_shape, activation='relu'),  # First hidden layer\n",
    "            Dense(32, activation='relu'),  # Second hidden layer\n",
    "            Dense(self.num_actions, activation='softmax')  # Output layer for 12 possible actions\n",
    "        ])\n",
    "\n",
    "        # Compile the model\n",
    "        self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'], )\n",
    "\n",
    "    def train(self, training_examples):\n",
    "                # Train the model (Assume more training data is available)\n",
    "        # For example purposes, we'll use the same sample data multiple times\n",
    "        X_train, X_test, y_train, y_test = custom_train_test_split(training_examples, test_size=0.3, shuffle=True, random_state=None)\n",
    "\n",
    "        # Convert target (y) to categorical (for classification)\n",
    "        num_actions = self.num_actions # Assuming 12 possible actions (0 to 11)\n",
    "        y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_actions)\n",
    "        y_test = tf.keras.utils.to_categorical(y_test, num_classes=num_actions)\n",
    "\n",
    "        callbacks = [EarlyStopping(patience=20, monitor='loss', verbose=0),\n",
    "             ReduceLROnPlateau(monitor='val_accuracy',factor=0.01, min_Ir=0.00001, verbose=0),\n",
    "             ModelCheckpoint('latest_policy_model.keras', verbose=0, save_best_only=True, save_weights_only=False)\n",
    "             ]\n",
    "\n",
    "        # Train the model (Assume more training data is available)\n",
    "        # For example purposes, we'll use the same sample data multiple times\n",
    "        self.model.fit(X_train, y_train, epochs=200, batch_size=64,  callbacks=callbacks, validation_data=(X_test,y_test))\n",
    "\n",
    "\n",
    "    def predict(self,data):\n",
    "        data = np.array(data)\n",
    "        prediction = self.model.predict(data, verbose=None)\n",
    "        return prediction[0]\n",
    "    \n",
    "\n",
    "class ValueModel:\n",
    "    def __init__(self):\n",
    "        self.X_shape= 14\n",
    "        self.model = Sequential([\n",
    "            Dense(64, input_dim=self.X_shape, activation='relu'),  # First hidden layer\n",
    "            Dense(32, activation='relu'),  # Second hidden layer\n",
    "            Dense(1)  # Output layer for 12 possible actions\n",
    "        ])\n",
    "\n",
    "        # Compile the model\n",
    "        self.model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "    def train(self, training_examples):\n",
    "        # Train the model (Assume more training data is available)\n",
    "        # For example purposes, we'll use the same sample data multiple times\n",
    "        X_train, X_test, y_train, y_test = custom_train_test_split(training_examples, test_size=0.3, shuffle=True, random_state=None)\n",
    "\n",
    "        callbacks = [EarlyStopping(patience=20, monitor='loss', verbose=0),\n",
    "             ReduceLROnPlateau(monitor='val_accuracy',factor=0.01, min_Ir=0.00001, verbose=0),\n",
    "             ModelCheckpoint('latest_value_model.keras', verbose=0, save_best_only=True, save_weights_only=False)\n",
    "             ]\n",
    "\n",
    "        self.model.fit(X_train, y_train, epochs=200, batch_size=64,  callbacks=callbacks, validation_data=(X_test,y_test))\n",
    "\n",
    "\n",
    "    def predict(self,data):\n",
    "        # Predict action for a new data point\n",
    "        data = np.array(data)\n",
    "        prediction = self.model.predict(data, verbose=None)\n",
    "        return prediction[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nwachukwu Ujubuonu\\scoop\\apps\\python\\current\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "value_model = ValueModel()\n",
    "policy_model = PolicyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_actions_mct(state):\n",
    "    board = state['board']\n",
    "    territory = state['player_territory'][1]\n",
    "    current_player = state['current_player']\n",
    "    valid_actions = []\n",
    "    for i,a in enumerate(board):\n",
    "        if current_player == 0 and i < territory and a != 0:\n",
    "                valid_actions.append(1)\n",
    "                continue\n",
    "        if current_player == 1 and i >= territory and a != 0:\n",
    "                valid_actions.append(1)\n",
    "                continue\n",
    "        valid_actions.append(0)\n",
    "    return valid_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self,state,prior,parent_node=None):\n",
    "        self.parent_node = parent_node\n",
    "        self.prior = prior\n",
    "        self.total_score = 0\n",
    "        self.visit_count = 0\n",
    "        self.expanded = False\n",
    "        self.children = {}\n",
    "        self.reward = state['reward']\n",
    "        self.state = state\n",
    "\n",
    "    def update_result(self, reward):\n",
    "        self.total_score += reward\n",
    "        self.visit_count += 1\n",
    "\n",
    "    def printer(self):\n",
    "        print('parent_node: ',self.parent_node)\n",
    "        print('prior: ',self.prior)\n",
    "        print('total_score: ',self.total_score)\n",
    "        print('visit_count: ',self.visit_count)\n",
    "        print('expanded: ',self.expanded)\n",
    "        print('children: ',self.children)\n",
    "        print('reward: ',self.reward)\n",
    "        print('state: ',self.state)\n",
    "\n",
    "\n",
    "\n",
    "def expand(node, action_probs):\n",
    "    \"\"\"\n",
    "    We expand a node and keep track of the prior policy probability given by neural network\n",
    "    \"\"\"\n",
    "    state = node.state\n",
    "    for action, prob in enumerate(action_probs):\n",
    "        if prob != 0:\n",
    "            new_state, new_reward = play(state, action)\n",
    "            # print(action, new_reward)\n",
    "            reward = assign_reward(state['reward'], new_reward)\n",
    "            new_state['reward'] = reward\n",
    "            node.children[action] = Node(state=new_state,prior=prob,parent_node=node)\n",
    "\n",
    "    node.expanded = True\n",
    "\n",
    "\n",
    "\n",
    "def resources_left( start_time, duration):\n",
    "    current_time = time.time()\n",
    "    if current_time - start_time >= duration:\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def ucb_score(parent, child):\n",
    "    \"\"\"\n",
    "    The score for an action that would transition between the parent and child.\n",
    "    \"\"\"\n",
    "    prior_score = child.prior * math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "    # prior_score = 2 * math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "    if child.visit_count > 0:\n",
    "        # The value of the child is from the perspective of the opposing player\n",
    "        value_score = -(child.total_score / child.visit_count)\n",
    "    else:\n",
    "        value_score = np.inf\n",
    "    return value_score + prior_score\n",
    "\n",
    "\n",
    "\n",
    "def select(node):\n",
    "    \"\"\"\n",
    "    Select the child with the highest UCB score.\n",
    "    \"\"\"\n",
    "    best_score = -np.inf\n",
    "    best_action = -1\n",
    "    best_child = None\n",
    "\n",
    "    for action, child in node.children.items():\n",
    "        score = ucb_score(node, child)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_action = action\n",
    "            best_child = child\n",
    "\n",
    "    return best_action, best_child\n",
    "\n",
    "\n",
    "\n",
    "def generate_action(state,agent_1, agent_2):\n",
    "   if state['current_player'] == 0:\n",
    "      func = agent_1['func']\n",
    "      arg = agent_1['arg']\n",
    "      return func(state, arg)\n",
    "\n",
    "   if state['current_player'] == 1:\n",
    "      func = agent_2['func']\n",
    "      arg = agent_2['arg']\n",
    "      return func(state, arg)\n",
    "\n",
    "\n",
    "\n",
    "def simulate_game(state,  agent_1, agent_2,show=False):\n",
    "    state = copy.deepcopy(state)\n",
    "    reward = [0,0,0]\n",
    "    path = []\n",
    "\n",
    "    while True:\n",
    "        action = generate_action(state, agent_1, agent_2)\n",
    "\n",
    "        if is_illegal_move(state, action):\n",
    "            continue\n",
    "\n",
    "        state, new_reward = play(state, action)\n",
    "        reward = assign_reward(reward, new_reward)\n",
    "        if show:\n",
    "            print_game_play(state, reward, action)\n",
    "\n",
    "        if end_game(state):\n",
    "            break\n",
    "        path.append(action)\n",
    "\n",
    "    return  (reward, path)\n",
    "\n",
    "\n",
    "\n",
    "def best_child(node):\n",
    "    actions = node.action_prob\n",
    "    highest_visit = -INFINITY\n",
    "    for action in actions:\n",
    "        child = node.children[action]\n",
    "        visit_count = child.visit_count\n",
    "        if visit_count > highest_visit:\n",
    "            highest_visit = visit_count\n",
    "            best_action = action\n",
    "    return best_action\n",
    "\n",
    "\n",
    "\n",
    "def back_propagation(node, result):\n",
    "    act_result = result * ((-1)**node.state['current_player'])\n",
    "    node.update_result(act_result)\n",
    "\n",
    "    parent_node = node.parent_node\n",
    "\n",
    "    if parent_node == None:\n",
    "        return\n",
    "\n",
    "    back_propagation(parent_node, result)\n",
    "\n",
    "\n",
    "\n",
    "def mcts(state,think_time):\n",
    "    start_time = time.time()\n",
    "    root = Node(state,state['current_player'])\n",
    "    data = [format_state(root.state)]\n",
    "    alpha = 0.6\n",
    "    action_probs = policy_model.predict(data)\n",
    "    valid_actions = np.array(get_valid_actions_mct(state))\n",
    "    action_probs = action_probs *valid_actions\n",
    "    action_probs /= np.sum(action_probs)\n",
    "    expand(root, action_probs)\n",
    "    i = 0    \n",
    "    \n",
    "    while resources_left(start_time, think_time):\n",
    "        node = root\n",
    "        while len(node.children) > 0:\n",
    "            _, child = select(node)\n",
    "            if child == None:\n",
    "                break\n",
    "            node = child\n",
    "\n",
    "        data = [format_state(node.state)]\n",
    "        if node.visit_count != 0:\n",
    "            action_probs = policy_model.predict(data)\n",
    "            valid_actions = np.array(get_valid_actions_mct(node.state))\n",
    "            action_probs = action_probs * valid_actions\n",
    "            action_probs /= np.sum(action_probs)\n",
    "            expand(node, action_probs)\n",
    "    \n",
    "        # node.printer()\n",
    "        value_reward = value_model.predict(data)\n",
    "        mcts_reward = norm(node.reward[0], node.reward[1])\n",
    "        value = ((1 - alpha)*mcts_reward)+(alpha * value_reward)\n",
    "        back_propagation(node, value)\n",
    "        i += 1\n",
    "\n",
    "    return root\n",
    "\n",
    "def select_action(node, temperature):\n",
    "        \"\"\"\n",
    "        Select action according to the visit count distribution and the temperature.\n",
    "        \"\"\"\n",
    "        visit_counts = np.array([child.visit_count for child in node.children.values()])\n",
    "        actions = [action for action in node.children.keys()]\n",
    "        if temperature == 0:\n",
    "            action = actions[np.argmax(visit_counts)]\n",
    "        elif temperature == float(\"inf\"):\n",
    "            action = np.random.choice(actions)\n",
    "        else:\n",
    "            # See paper appendix Data Generation\n",
    "            visit_count_distribution = visit_counts ** (1 / temperature)\n",
    "            visit_count_distribution = visit_count_distribution / sum(visit_count_distribution)\n",
    "            action = np.random.choice(actions, p=visit_count_distribution)\n",
    "\n",
    "        return action\n",
    "\n",
    "def agent(state, arg):\n",
    "    think_time = arg['think_time']\n",
    "    temperature = arg['temperature']\n",
    "    root = mcts(state, think_time)\n",
    "    return select_action(root, temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = {\n",
    "   'board' :[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
    "   'current_player': 0,\n",
    "   'player_territory': (0,6),\n",
    "   'reward':[0,0,0],\n",
    "}\n",
    "# state ={'board': [0, 0, 1, 0, 3, 0, 9, 2, 1, 10, 1, 9], 'current_player': 1, 'player_territory': (0, 6), 'reward': [8, 4, 0]}\n",
    "\n",
    "root = mcts(state, 5)\n",
    "action = select_action(root, 0)\n",
    "action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_episode(state,  agent_1, agent_2,show=False):\n",
    "    state = copy.deepcopy(state)\n",
    "    state['reward'] = [0,0,0]\n",
    "    reward = [0,0,0]\n",
    "    path = []\n",
    "    train_example_policy = []\n",
    "    train_example_value = []\n",
    "\n",
    "    while True:\n",
    "        action = generate_action(state, agent_1, agent_2)\n",
    "        \n",
    "        if is_illegal_move(state, action):\n",
    "            continue\n",
    "\n",
    "        train_example_policy.append([*format_state(state), action])\n",
    "        state, new_reward = play(state, action)\n",
    "        reward = assign_reward(reward, new_reward)\n",
    "        state['reward'] = reward\n",
    "        path.append(action)\n",
    "\n",
    "        if show:\n",
    "            print_game_play(state, reward, action)\n",
    "\n",
    "        if end_game(state) or len(path)> 20:\n",
    "            value = norm(reward[0],reward[1])\n",
    "            for hist_state in train_example_policy:\n",
    "                player_value = value * ((-1) ** (hist_state[12]))\n",
    "                train_example_value.append([*hist_state[0:14], player_value])\n",
    "                \n",
    "            break\n",
    "\n",
    "    return  train_example_value, train_example_policy, path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(num_of_eps):\n",
    "    value_data = []\n",
    "    policy_data = []\n",
    "\n",
    "    state = {\n",
    "   'board' :[4,4,4,4,4,4,4,4,4,4,4,4],\n",
    "   'current_player': 0,\n",
    "   'player_territory': (0,6)\n",
    "    }\n",
    "    \n",
    "    agent_1 = {\n",
    "    'func': agent,\n",
    "    'arg': {\n",
    "        'think_time': 5,\n",
    "        'temperature': 1,\n",
    "    },\n",
    "    'name': 'mcts_agent',\n",
    "    'elo': 1200\n",
    "    }\n",
    "\n",
    "    agent_2 = {\n",
    "    'func': agent,\n",
    "    'arg': {\n",
    "        'think_time': 5,\n",
    "        'temperature': 1,\n",
    "    },\n",
    "    'name': 'mcts_agent',\n",
    "    'elo': 1200\n",
    "    }\n",
    "\n",
    "    for i in range(num_of_eps):\n",
    "        state['current_player'] = i%2\n",
    "        state_value_data, state_policy_data, path = execute_episode(state,  agent_1, agent_2)\n",
    "        add_data_to_csv(file_name='value_data.csv', new_data=state_value_data, y_column='value')\n",
    "        add_data_to_csv(file_name='policy_data.csv', new_data=state_policy_data, y_column='policy')\n",
    "        value_data.extend(state_value_data)\n",
    "        policy_data.extend(state_policy_data)\n",
    "        \n",
    "        print('path_len: ', len(path))\n",
    "        print('path: ', path)\n",
    "        print(((i+1)/num_of_eps) * 100,'%')\n",
    "    \n",
    "        \n",
    "    return value_data, policy_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_play(num_of_eps):\n",
    "    value_data = []\n",
    "    policy_data = []\n",
    "\n",
    "    state = {\n",
    "   'board' :[4,4,4,4,4,4,4,4,4,4,4,4],\n",
    "   'current_player': 0,\n",
    "   'player_territory': (0,6)\n",
    "    }\n",
    "    \n",
    "    agent_1 = {\n",
    "    'func': agent,\n",
    "    'arg': {\n",
    "        'max_iterations': 100,\n",
    "        'temperature': 0,\n",
    "    },\n",
    "    'name': 'mcts_agent',\n",
    "    'elo': 1200\n",
    "    }\n",
    "\n",
    "    agent_2 = {\n",
    "    'func': minimax_agent,\n",
    "    'arg': {\n",
    "        'max_dept': 3,\n",
    "    },\n",
    "    'name': 'minimax_agent',\n",
    "    'elo': 1200\n",
    "    }\n",
    "\n",
    "    for i in range(num_of_eps):\n",
    "        state['current_player'] = i%2\n",
    "        state_value_data, state_policy_data, path = execute_episode(state,  agent_1, agent_2, True)\n",
    "        print('path_len: ', len(path))\n",
    "        print('path: ', path)\n",
    "        value_data.extend(state_value_data)\n",
    "        policy_data.extend(state_policy_data)\n",
    "        \n",
    "    return value_data, policy_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 100\n",
    "data = [\n",
    "    [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 6],# 0\n",
    "]\n",
    "\n",
    "for _ in range(num):\n",
    "    value_data, policy_data = generate_training_data(10)\n",
    "    print()\n",
    "    value_model.train(value_data)\n",
    "    policy_model.train(policy_data)\n",
    "    v = value_model.predict(data)\n",
    "    print('value', v)\n",
    "    p = policy_model.predict(data)\n",
    "    print('policy', np.argmax(p))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.339334\n"
     ]
    }
   ],
   "source": [
    "model = value_model\n",
    "data = [\n",
    "    [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 6],# 0\n",
    "    [0,2,3,0,1,0,0,0,1,0,16,1,0,6],# 1\n",
    "    [0,0,2,7,2,7,7,1,2,0,8,8,0,6],# 1\n",
    "]\n",
    "# data = [[0, 0, 3, 11, 1, 1, 0, 1, 2, 11, 1, 1, 1, 6]]\n",
    "# data = np.array(data)\n",
    "p = model.predict(data)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0497162\n",
    "# -1.2499301"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
