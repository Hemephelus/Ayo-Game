{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "# from tensorflow.keras import callbacks\n",
    " \n",
    "\n",
    "import copy\n",
    "import random\n",
    "from json.encoder import INFINITY\n",
    "import pandas as pd\n",
    "import math\n",
    "from ayo_game import play, is_illegal_move, assign_reward, print_game_play, end_game\n",
    "from agents import random_agent as ra\n",
    "from agents import minimax_agent as ma\n",
    "from agents import mcts_agent as mctsa\n",
    "seed = 37\n",
    "\n",
    "randint = random.randint\n",
    "random.seed(seed)\n",
    "value_model_file = 'latest_value_model.keras'\n",
    "policy_model_file = 'latest_policy_model.keras'\n",
    "# checkpoint_filepath = 'latest.keras'\n",
    "# checkpoint_filepath = 'multi.keras'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimax_agent = ma.agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_state(state):\n",
    "    x = []\n",
    "    x.extend(state['board'])\n",
    "    x.append(state['current_player'])\n",
    "    x.append(state['player_territory'][1])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_train_test_split(data, test_size=0.2, shuffle=True, random_state=None):\n",
    "    \"\"\"\n",
    "    Custom function to split data into training and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input features (numpy array or list)\n",
    "    - y: Target labels (numpy array or list)\n",
    "    - test_size: Proportion of the data to include in the test split (default 0.2)\n",
    "    - shuffle: Whether to shuffle the data before splitting (default True)\n",
    "    - random_state: Seed for the random number generator (optional, for reproducibility)\n",
    "\n",
    "    Returns:\n",
    "    - X_train, X_test, y_train, y_test: Split training and testing data\n",
    "    \"\"\"\n",
    "    X = np.array([d[0] for d in data])\n",
    "    y = np.array([d[1] for d in data])\n",
    "\n",
    "    # Set random seed if provided (to ensure reproducibility)\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    # Get the number of samples\n",
    "    num_samples = len(X)\n",
    "\n",
    "    # Shuffle the data if requested\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(num_samples)\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "\n",
    "    # Compute the split index\n",
    "    split_index = int(num_samples * (1 - test_size))\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = X[:split_index], X[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_duplicates(arr):\n",
    "    # Convert the list of lists to a NumPy array\n",
    "    np_arr = np.array(arr, dtype=object)\n",
    "    \n",
    "    # Create a set to keep track of unique first elements (list of numbers)\n",
    "    seen = set()\n",
    "    unique_arr = []\n",
    "\n",
    "    for item in np_arr:\n",
    "        # Convert the list of numbers (first element) to a tuple so it can be added to a set\n",
    "        num_tuple = tuple(item[0])\n",
    "        if num_tuple not in seen:\n",
    "            unique_arr.append(item)\n",
    "            seen.add(num_tuple)\n",
    "    \n",
    "    return np.array(unique_arr, dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyModel:\n",
    "    def __init__(self):\n",
    "        self.num_actions=12\n",
    "        self.X_shape=14\n",
    "        self.model = Sequential([\n",
    "            Dense(64, input_dim=self.X_shape, activation='relu'),  # First hidden layer\n",
    "            Dense(32, activation='relu'),  # Second hidden layer\n",
    "            Dense(self.num_actions, activation='softmax')  # Output layer for 12 possible actions\n",
    "        ])\n",
    "\n",
    "        # Compile the model\n",
    "        self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'], )\n",
    "\n",
    "    def train(self, training_examples):\n",
    "                # Train the model (Assume more training data is available)\n",
    "        # For example purposes, we'll use the same sample data multiple times\n",
    "        X_train, X_test, y_train, y_test = custom_train_test_split(training_examples, test_size=0.3, shuffle=True, random_state=None)\n",
    "\n",
    "        # Convert target (y) to categorical (for classification)\n",
    "        num_actions = self.num_actions # Assuming 12 possible actions (0 to 11)\n",
    "        y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_actions)\n",
    "        y_test = tf.keras.utils.to_categorical(y_test, num_classes=num_actions)\n",
    "\n",
    "        callbacks = [EarlyStopping(patience=20, monitor='loss', verbose=0),\n",
    "             ReduceLROnPlateau(monitor='val_accuracy',factor=0.01, min_Ir=0.00001, verbose=0),\n",
    "             ModelCheckpoint('latest_policy_model.keras', verbose=0, save_best_only=True, save_weights_only=False)]\n",
    "\n",
    "        # Train the model (Assume more training data is available)\n",
    "        # For example purposes, we'll use the same sample data multiple times\n",
    "        self.model.fit(X_train, y_train, epochs=200, batch_size=64,  callbacks=callbacks, validation_data=(X_test,y_test))\n",
    "\n",
    "\n",
    "    def predict(self,data):\n",
    "        data = np.array(data)\n",
    "        prediction = self.model.predict(data, verbose=None)\n",
    "        return prediction[0]\n",
    "    \n",
    "\n",
    "class ValueModel:\n",
    "    def __init__(self):\n",
    "        self.X_shape= 14\n",
    "        self.model = Sequential([\n",
    "            Dense(64, input_dim=self.X_shape, activation='relu'),  # First hidden layer\n",
    "            Dense(32, activation='relu'),  # Second hidden layer\n",
    "            Dense(1)  # Output layer for 12 possible actions\n",
    "        ])\n",
    "\n",
    "        # Compile the model\n",
    "        self.model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "    def train(self, training_examples):\n",
    "        # Train the model (Assume more training data is available)\n",
    "        # For example purposes, we'll use the same sample data multiple times\n",
    "        X_train, X_test, y_train, y_test = custom_train_test_split(training_examples, test_size=0.3, shuffle=True, random_state=None)\n",
    "\n",
    "        callbacks = [EarlyStopping(patience=20, monitor='loss', verbose=0),\n",
    "             ReduceLROnPlateau(monitor='val_accuracy',factor=0.01, min_Ir=0.00001, verbose=0),\n",
    "             ModelCheckpoint('latest_value_model.keras', verbose=0, save_best_only=True, save_weights_only=False)]\n",
    "\n",
    "        self.model.fit(X_train, y_train, epochs=200, batch_size=64,  callbacks=callbacks, validation_data=(X_test,y_test))\n",
    "\n",
    "\n",
    "    def predict(self,data):\n",
    "        # Predict action for a new data point\n",
    "        data = np.array(data)\n",
    "        prediction = self.model.predict(data, verbose=None)\n",
    "        return prediction[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_model = ValueModel()\n",
    "policy_model = PolicyModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_valid_actions_mct(state):\n",
    "    board = state['board']\n",
    "    territory = state['player_territory'][1]\n",
    "    current_player = state['current_player']\n",
    "    valid_actions = []\n",
    "    for i,a in enumerate(board):\n",
    "        if current_player == 0 and i < territory and a != 0:\n",
    "                valid_actions.append(1)\n",
    "                continue\n",
    "        if current_player == 1 and i >= territory and a != 0:\n",
    "                valid_actions.append(1)\n",
    "                continue\n",
    "        valid_actions.append(0)\n",
    "    return valid_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self,state,prior,parent_node=None):\n",
    "        self.parent_node = parent_node\n",
    "        self.prior = prior\n",
    "        self.total_score = 0\n",
    "        self.visit_count = 0\n",
    "        self.expanded = False\n",
    "        self.children = {}\n",
    "        self.state = state\n",
    "\n",
    "    def update_result(self, reward):\n",
    "        self.total_score += reward\n",
    "        self.visit_count += 1\n",
    "\n",
    "    def printer(self):\n",
    "        print('parent_node: ',self.parent_node)\n",
    "        print('total_score: ',self.total_score)\n",
    "        print('visit_count: ',self.visit_count)\n",
    "        print('expanded: ',self.expanded)\n",
    "        print('children: ',self.children)\n",
    "        print('state: ',self.state)\n",
    "\n",
    "\n",
    "\n",
    "def expand(node, action_probs):\n",
    "    \"\"\"\n",
    "    We expand a node and keep track of the prior policy probability given by neural network\n",
    "    \"\"\"\n",
    "    state = node.state\n",
    "    for action, prob in enumerate(action_probs):\n",
    "        if prob != 0:\n",
    "            new_state, _ = play(state, action)\n",
    "            node.children[action] = Node(state=new_state,prior=prob,parent_node=node)\n",
    "\n",
    "    node.expanded = True\n",
    "\n",
    "\n",
    "\n",
    "def resources_left( max_iterations, iterations):\n",
    "    return max_iterations > iterations\n",
    "\n",
    "\n",
    "def ucb_score(parent, child):\n",
    "    \"\"\"\n",
    "    The score for an action that would transition between the parent and child.\n",
    "    \"\"\"\n",
    "    prior_score = child.prior * math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "    if child.visit_count > 0:\n",
    "        # The value of the child is from the perspective of the opposing player\n",
    "        value_score = child.total_score / child.visit_count\n",
    "    else:\n",
    "        value_score = np.inf\n",
    "    return value_score + prior_score\n",
    "\n",
    "\n",
    "\n",
    "def select(node):\n",
    "    \"\"\"\n",
    "    Select the child with the highest UCB score.\n",
    "    \"\"\"\n",
    "    best_score = -np.inf\n",
    "    best_action = -1\n",
    "    best_child = None\n",
    "\n",
    "    for action, child in node.children.items():\n",
    "        score = ucb_score(node, child)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_action = action\n",
    "            best_child = child\n",
    "\n",
    "    return best_action, best_child\n",
    "\n",
    "\n",
    "def generate_action(state,agent_1, agent_2):\n",
    "   if state['current_player'] == 0:\n",
    "      func = agent_1['func']\n",
    "      arg = agent_1['arg']\n",
    "      return func(state, arg)\n",
    "\n",
    "   if state['current_player'] == 1:\n",
    "      func = agent_2['func']\n",
    "      arg = agent_2['arg']\n",
    "      return func(state, arg)\n",
    "\n",
    "\n",
    "\n",
    "def simulate_game(state,  agent_1, agent_2,show=False):\n",
    "    state = copy.deepcopy(state)\n",
    "    reward = [0,0,0]\n",
    "    path = []\n",
    "\n",
    "    while True:\n",
    "        action = generate_action(state, agent_1, agent_2)\n",
    "\n",
    "        if is_illegal_move(state, action):\n",
    "            continue\n",
    "\n",
    "        state, new_reward = play(state, action)\n",
    "        reward = assign_reward(reward, new_reward)\n",
    "        if show:\n",
    "            print_game_play(state, reward, action)\n",
    "\n",
    "        if end_game(state):\n",
    "            break\n",
    "        path.append(action)\n",
    "\n",
    "    return  (reward, path)\n",
    "\n",
    "\n",
    "\n",
    "def best_child(node):\n",
    "    actions = node.action_prob\n",
    "    highest_visit = -INFINITY\n",
    "    for action in actions:\n",
    "        child = node.children[action]\n",
    "        visit_count = child.visit_count\n",
    "        if visit_count > highest_visit:\n",
    "            highest_visit = visit_count\n",
    "            best_action = action\n",
    "    return best_action\n",
    "\n",
    "\n",
    "\n",
    "def back_propagation(node, result):\n",
    "    act_result = result * ((-1)**node.state['current_player'])\n",
    "    node.update_result(act_result)\n",
    "\n",
    "    parent_node = node.parent_node\n",
    "\n",
    "    if parent_node == None:\n",
    "        return\n",
    "\n",
    "    back_propagation(parent_node, result)\n",
    "\n",
    "\n",
    "\n",
    "def mcts(state,max_iterations):\n",
    "    root = Node(state,state['current_player'])\n",
    "    data = [format_state(root.state)]\n",
    "    action_probs = policy_model.predict(data)\n",
    "    valid_actions = np.array(get_valid_actions_mct(state))\n",
    "    action_probs = action_probs *valid_actions\n",
    "    action_probs /= np.sum(action_probs)\n",
    "    expand(root, action_probs)\n",
    "    i = 0    \n",
    "    \n",
    "    while resources_left(max_iterations, i):\n",
    "        node = root\n",
    "        while len(node.children) > 0:\n",
    "            _, child = select(node)\n",
    "            if child == None:\n",
    "                break\n",
    "            node = child\n",
    "\n",
    "\n",
    "        data = [format_state(node.state)]\n",
    "        if node.visit_count != 0:\n",
    "            action_probs = policy_model.predict(data)\n",
    "            valid_actions = np.array(get_valid_actions_mct(node.state))\n",
    "            action_probs = action_probs * valid_actions\n",
    "            action_probs /= np.sum(action_probs)\n",
    "            expand(node, action_probs)\n",
    "    \n",
    "        result = value_model.predict(data)\n",
    "        # print(result,data)\n",
    "        back_propagation(node, result)\n",
    "        i += 1\n",
    "\n",
    "    return root\n",
    "\n",
    "def select_action(node, temperature):\n",
    "        \"\"\"\n",
    "        Select action according to the visit count distribution and the temperature.\n",
    "        \"\"\"\n",
    "        visit_counts = np.array([child.visit_count for child in node.children.values()])\n",
    "        actions = [action for action in node.children.keys()]\n",
    "        if temperature == 0:\n",
    "            action = actions[np.argmax(visit_counts)]\n",
    "        elif temperature == float(\"inf\"):\n",
    "            action = np.random.choice(actions)\n",
    "        else:\n",
    "            # See paper appendix Data Generation\n",
    "            visit_count_distribution = visit_counts ** (1 / temperature)\n",
    "            visit_count_distribution = visit_count_distribution / sum(visit_count_distribution)\n",
    "            action = np.random.choice(actions, p=visit_count_distribution)\n",
    "\n",
    "        return action\n",
    "\n",
    "def agent(state, arg):\n",
    "    max_iterations = arg['max_iterations']\n",
    "    temperature = arg['temperature']\n",
    "    root = mcts(state, max_iterations)\n",
    "    return select_action(root, temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "   'board' :[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\n",
    "   'current_player': 1,\n",
    "   'player_territory': (0,6)\n",
    "}\n",
    "\n",
    "root = mcts(state, 10)\n",
    "action = select_action(root, 0)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.printer()\n",
    "child = root.children[11]\n",
    "child.printer()\n",
    "child.total_score/child.visit_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_episode(state,  agent_1, agent_2,show=False):\n",
    "    state = copy.deepcopy(state)\n",
    "    reward = [0,0,0]\n",
    "    path = []\n",
    "    train_example_policy = []\n",
    "    train_example_value = []\n",
    "\n",
    "    while True:\n",
    "        action = generate_action(state, agent_1, agent_2)\n",
    "        \n",
    "        if is_illegal_move(state, action):\n",
    "            continue\n",
    "\n",
    "        train_example_policy.append([format_state(state), action])\n",
    "        state, new_reward = play(state, action)\n",
    "        reward = assign_reward(reward, new_reward)\n",
    "\n",
    "        if show:\n",
    "            print_game_play(state, reward, action)\n",
    "\n",
    "        if end_game(state):\n",
    "            diff = reward[0] - reward[1]\n",
    "\n",
    "            if diff < 0:\n",
    "                value = -1\n",
    "            elif diff > 0:\n",
    "                value = 1\n",
    "            else:\n",
    "                value = 0\n",
    "\n",
    "            for hist_state, action in train_example_policy:\n",
    "                player_value = value * ((-1) ** (hist_state[12]))\n",
    "                train_example_value.append([hist_state, player_value])\n",
    "                \n",
    "            break\n",
    "        path.append(action)\n",
    "\n",
    "    return  train_example_value, train_example_policy, path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(num_of_eps):\n",
    "    value_data = []\n",
    "    policy_data = []\n",
    "\n",
    "    state = {\n",
    "   'board' :[4,4,4,4,4,4,4,4,4,4,4,4],\n",
    "   'current_player': 0,\n",
    "   'player_territory': (0,6)\n",
    "    }\n",
    "    \n",
    "    agent_1 = {\n",
    "    'func': agent,\n",
    "    'arg': {\n",
    "        'max_iterations': 100,\n",
    "        'temperature': 1,\n",
    "    },\n",
    "    'name': 'mcts_agent',\n",
    "    'elo': 1200\n",
    "    }\n",
    "\n",
    "    agent_2 = {\n",
    "    'func': agent,\n",
    "    'arg': {\n",
    "        'max_iterations': 100,\n",
    "        'temperature': 1,\n",
    "    },\n",
    "    'name': 'mcts_agent',\n",
    "    'elo': 1200\n",
    "    }\n",
    "\n",
    "    for i in range(num_of_eps):\n",
    "        state_value_data, state_policy_data, path = execute_episode(state,  agent_1, agent_2)\n",
    "        add_data_to_csv(file_name='value_data.csv', new_data=state_value_data, y_column='value')\n",
    "        add_data_to_csv(file_name='policy_data.csv', new_data=state_policy_data, y_column='policy')\n",
    "        value_data.extend(state_value_data)\n",
    "        policy_data.extend(state_policy_data)\n",
    "        \n",
    "        print('path: ', path)\n",
    "        print((i/num_of_eps) * 100,'%')\n",
    "    \n",
    "        \n",
    "    return remove_duplicates(value_data), remove_duplicates(policy_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_play(num_of_eps):\n",
    "    value_data = []\n",
    "    policy_data = []\n",
    "\n",
    "    state = {\n",
    "   'board' :[4,4,4,4,4,4,4,4,4,4,4,4],\n",
    "   'current_player': 0,\n",
    "   'player_territory': (0,6)\n",
    "    }\n",
    "    \n",
    "    agent_1 = {\n",
    "    'func': agent,\n",
    "    'arg': {\n",
    "        'max_iterations': 100,\n",
    "        'temperature': 0,\n",
    "    },\n",
    "    'name': 'mcts_agent',\n",
    "    'elo': 1200\n",
    "    }\n",
    "\n",
    "    agent_2 = {\n",
    "    'func': minimax_agent,\n",
    "    'arg': {\n",
    "        'max_dept': 3,\n",
    "    },\n",
    "    'name': 'minimax_agent',\n",
    "    'elo': 1200\n",
    "    }\n",
    "\n",
    "    for i in range(num_of_eps):\n",
    "        state['current_player'] = i%2\n",
    "        state_value_data, state_policy_data, path = execute_episode(state,  agent_1, agent_2, True)\n",
    "        print('state_value_data: ', state_value_data)\n",
    "        print('state_policy_data: ', state_policy_data)\n",
    "        print('path: ', path)\n",
    "        value_data.extend(state_value_data)\n",
    "        policy_data.extend(state_policy_data)\n",
    "        \n",
    "    return remove_duplicates(value_data), remove_duplicates(policy_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 100\n",
    "for _ in range(num):\n",
    "    value_data, policy_data = generate_training_data(100)\n",
    "    value_model.train(value_data)\n",
    "    policy_model.train(policy_data)\n",
    "    test_play(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path:  [0, 6, 0, 10, 2, 11, 1, 7, 5, 8, 5, 8, 0, 7, 2, 9, 5, 9, 1, 11, 4, 8, 5, 11, 0, 8, 2, 9, 0, 11, 0, 8, 2, 6, 3, 10, 0, 6, 1, 9, 3, 6, 4, 9]\n",
    "# path:  [3, 7, 1, 6, 0, 11, 3, 9, 0, 8, 5, 10, 4, 10, 1, 9, 0, 10, 2, 11, 0, 10, 2, 6, 0, 10, 2, 11, 0, 8, 1, 10, 2, 7, 0, 8, 4, 11, 0, 10, 0, 11, 2, 8, 0, 9, 1, 10, 3, 6, 2, 11, 3, 6, 0, 9, 1, 7, 0, 10, 2, 9, 0, 11, 0, 10, 2, 11, 0, 6, 4, 9, 5, 9, 3, 11, 0, 6, 5, 8, 2, 11, 5, 10, 2, 8, 1, 11, 0, 9, 1, 9, 3, 11, 2, 8, 5, 9]\n",
    "# path:  [2, 9, 5, 11, 3, 9, 0, 8]\n",
    "# path:  [4, 9, 3, 8, 2, 11, 0, 8, 5, 10, 0, 9, 5, 11, 0, 9, 5, 11, 5, 8, 3, 6, 1, 8, 2, 8, 1, 6, 2, 10, 0, 7, 1, 11, 2, 8, 0, 7, 3, 6, 4, 8, 0, 11, 0, 10, 0, 8, 4, 10, 1, 11, 0, 6, 2, 7, 5, 6, 1, 9, 2, 10, 5, 11, 0, 6, 1, 9, 2, 10]\n",
    "# path:  [3, 7, 2, 10, 0, 10, 0, 9, 2, 11, 0, 6, 3, 9, 1, 7, 3, 6, 2, 8]\n",
    "# path:  [3, 7, 1, 6, 0, 10, 0, 10, 1, 10, 0, 11]\n",
    "# path:  [3, 7, 1, 6, 0, 11, 1, 9, 0, 11, 1, 7, 0, 6, 4, 7, 1, 9, 0, 10, 1, 7, 2, 8, 0, 10, 3, 7, 1, 11, 4, 10, 2, 9, 3, 10, 0, 9, 1, 10, 4, 10, 2, 7, 0, 8, 2, 10, 0, 8, 2, 6]\n",
    "# path:  [3, 7, 4, 6, 0, 10, 2, 8, 0, 11, 2, 9, 0, 10, 4, 11, 3, 8, 0, 7, 2, 8]\n",
    "# path:  [0, 6, 0, 10, 2, 11, 1, 7, 1, 8, 3, 6, 5, 7, 3, 7, 4, 6, 3, 11, 2, 10, 0, 11, 1, 6, 0, 8, 5, 11, 0, 9, 4, 8, 0, 6, 4, 6, 3, 7, 0, 9, 2, 10, 3, 8, 5, 8, 4, 7, 1, 8, 2, 9, 5]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
