{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "# from tensorflow.keras import callbacks\n",
    " \n",
    "\n",
    "import copy\n",
    "import random\n",
    "from json.encoder import INFINITY\n",
    "import pandas as pd\n",
    "import math\n",
    "from ayo_game import play, is_illegal_move, assign_reward, print_game_play, end_game\n",
    "from agents import random_agent as ra\n",
    "from agents import minimax_agent as ma\n",
    "from agents import mcts_agent as mctsa\n",
    "seed = 37\n",
    "\n",
    "randint = random.randint\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_agent = ra.agent\n",
    "minimax_agent = ma.agent\n",
    "minimax_value = ma.get_action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_train_test_split(data, test_size=0.2, shuffle=True, random_state=None):\n",
    "    \"\"\"\n",
    "    Custom function to split data into training and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input features (numpy array or list)\n",
    "    - y: Target labels (numpy array or list)\n",
    "    - test_size: Proportion of the data to include in the test split (default 0.2)\n",
    "    - shuffle: Whether to shuffle the data before splitting (default True)\n",
    "    - random_state: Seed for the random number generator (optional, for reproducibility)\n",
    "\n",
    "    Returns:\n",
    "    - X_train, X_test, y_train, y_test: Split training and testing data\n",
    "    \"\"\"\n",
    "    X = np.array([d[0] for d in data])\n",
    "    y = np.array([d[1] for d in data])\n",
    "\n",
    "    # Set random seed if provided (to ensure reproducibility)\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    # Get the number of samples\n",
    "    num_samples = len(X)\n",
    "\n",
    "    # Shuffle the data if requested\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(num_samples)\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "\n",
    "    # Compute the split index\n",
    "    split_index = int(num_samples * (1 - test_size))\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test = X[:split_index], X[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyModel:\n",
    "    def __init__(self):\n",
    "        self.num_actions=12\n",
    "        self.X_shape=14\n",
    "        self.model = Sequential([\n",
    "            Dense(64, input_dim=self.X_shape, activation='relu'),  # First hidden layer\n",
    "            Dense(32, activation='relu'),  # Second hidden layer\n",
    "            Dense(self.num_actions, activation='softmax')  # Output layer for 12 possible actions\n",
    "        ])\n",
    "\n",
    "        # Compile the model\n",
    "        self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'], )\n",
    "\n",
    "    def train(self, training_examples):\n",
    "                # Train the model (Assume more training data is available)\n",
    "        # For example purposes, we'll use the same sample data multiple times\n",
    "        X_train, X_test, y_train, y_test = custom_train_test_split(training_examples, test_size=0.3, shuffle=True, random_state=None)\n",
    "\n",
    "        # Convert target (y) to categorical (for classification)\n",
    "        num_actions = self.num_actions # Assuming 12 possible actions (0 to 11)\n",
    "        y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_actions)\n",
    "        y_test = tf.keras.utils.to_categorical(y_test, num_classes=num_actions)\n",
    "\n",
    "        callbacks = [EarlyStopping(patience=20, monitor='loss', verbose=0),\n",
    "             ReduceLROnPlateau(monitor='val_accuracy',factor=0.01, min_Ir=0.00001, verbose=0),\n",
    "             ModelCheckpoint('latest_policy_model.keras', verbose=0, save_best_only=True, save_weights_only=False)]\n",
    "\n",
    "        # Train the model (Assume more training data is available)\n",
    "        # For example purposes, we'll use the same sample data multiple times\n",
    "        self.model.fit(X_train, y_train, epochs=200, batch_size=64,  callbacks=callbacks, validation_data=(X_test,y_test))\n",
    "\n",
    "\n",
    "    def predict(self,data):\n",
    "        prediction = self.model.predict(data)\n",
    "        return prediction\n",
    "    \n",
    "\n",
    "class ValueModel:\n",
    "    def __init__(self):\n",
    "        self.X_shape= 14\n",
    "        self.model = Sequential([\n",
    "            Dense(64, input_dim=self.X_shape, activation='relu'),  # First hidden layer\n",
    "            Dense(32, activation='relu'),  # Second hidden layer\n",
    "            Dense(1)  # Output layer for 12 possible actions\n",
    "        ])\n",
    "\n",
    "        # Compile the model\n",
    "        self.model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "    def train(self, training_examples):\n",
    "        # Train the model (Assume more training data is available)\n",
    "        # For example purposes, we'll use the same sample data multiple times\n",
    "        X_train, X_test, y_train, y_test = custom_train_test_split(training_examples, test_size=0.3, shuffle=True, random_state=None)\n",
    "\n",
    "        callbacks = [EarlyStopping(patience=20, monitor='loss', verbose=0),\n",
    "             ReduceLROnPlateau(monitor='val_accuracy',factor=0.01, min_Ir=0.00001, verbose=0),\n",
    "             ModelCheckpoint('latest_value_model.keras', verbose=0, save_best_only=True, save_weights_only=False)]\n",
    "\n",
    "        self.model.fit(X_train, y_train, epochs=200, batch_size=64,  callbacks=callbacks, validation_data=(X_test,y_test))\n",
    "\n",
    "\n",
    "    def predict(self,data):\n",
    "        # Predict action for a new data point\n",
    "        data = np.array(data)\n",
    "        prediction = self.model.predict(data)\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model = PolicyModel()\n",
    "value_model = ValueModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_state(state):\n",
    "    x = []\n",
    "    x.extend(state['board'])\n",
    "    x.append(state['current_player'])\n",
    "    x.append(state['player_territory'][1])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_duplicates(arr):\n",
    "    # Convert the list of lists to a NumPy array\n",
    "    np_arr = np.array(arr, dtype=object)\n",
    "    \n",
    "    # Create a set to keep track of unique first elements (list of numbers)\n",
    "    seen = set()\n",
    "    unique_arr = []\n",
    "\n",
    "    for item in np_arr:\n",
    "        # Convert the list of numbers (first element) to a tuple so it can be added to a set\n",
    "        num_tuple = tuple(item[0])\n",
    "        if num_tuple not in seen:\n",
    "            unique_arr.append(item)\n",
    "            seen.add(num_tuple)\n",
    "    \n",
    "    return np.array(unique_arr, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_action(state,agent_1, agent_2):\n",
    "   if state['current_player'] == 0:\n",
    "      func = agent_1['func']\n",
    "      arg = agent_1['arg']\n",
    "      return func(state, arg)\n",
    "\n",
    "   if state['current_player'] == 1:\n",
    "      func = agent_2['func']\n",
    "      arg = agent_2['arg']\n",
    "      return func(state, arg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_episode(state,  agent_1, agent_2,show=False):\n",
    "    state = copy.deepcopy(state)\n",
    "    reward = [0,0,0]\n",
    "    path = []\n",
    "    train_example_policy = []\n",
    "    train_example_value = []\n",
    "\n",
    "    while True:\n",
    "        action = generate_action(state, agent_1, agent_2)\n",
    "        \n",
    "        if is_illegal_move(state, action):\n",
    "            continue\n",
    "\n",
    "        _, value = minimax_value(state, agent_2['arg'])\n",
    "        train_example_value.append([format_state(state), value])\n",
    "        train_example_policy.append([format_state(state), action])\n",
    "        state, new_reward = play(state, action)\n",
    "        reward = assign_reward(reward, new_reward)\n",
    "        if show:\n",
    "            print_game_play(state, reward, action)\n",
    "\n",
    "        if end_game(state):\n",
    "            break\n",
    "        path.append(action)\n",
    "\n",
    "    return  train_example_value, train_example_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data():\n",
    "    num_of_eps = 100\n",
    "    value_data = []\n",
    "    policy_data = []\n",
    "    state = {\n",
    "   # 'board' :[6, 6, 2, 7, 1, 6, 1, 6, 6, 6, 0, 1],\n",
    "   'board' :[4,4,4,4,4,4,4,4,4,4,4,4],\n",
    "   'current_player': 0,\n",
    "   'player_territory': (0,6)\n",
    "    }\n",
    "    \n",
    "    agent_1 = {\n",
    "    'func': random_agent,\n",
    "    'arg': {},\n",
    "    'name': 'random_agent',\n",
    "    'elo': 1200\n",
    "    }\n",
    "        \n",
    "    agent_2 = {\n",
    "    'func': minimax_agent,\n",
    "    'arg': {\n",
    "        'max_dept': 9,\n",
    "    },\n",
    "    'name': 'minimax_agent_dept_3',\n",
    "    'elo': 1200\n",
    "    }\n",
    "\n",
    "    for i in range(num_of_eps):\n",
    "        state['current_player'] = i%2\n",
    "        state_value_data, state_policy_data = execute_episode(state,  agent_1, agent_2)\n",
    "        print('state_value_data: ', state_value_data)\n",
    "        print('state_policy_data: ', state_policy_data)\n",
    "        value_data.extend(state_value_data)\n",
    "        policy_data.extend(state_policy_data)\n",
    "        print((i/num_of_eps) * 100,'%')\n",
    "        \n",
    "    return remove_duplicates(value_data), remove_duplicates(policy_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_value_data:  [[[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 6], 0], [[1, 6, 6, 2, 7, 1, 6, 1, 6, 6, 6, 0, 1, 6], -8], [[0, 8, 8, 0, 0, 2, 7, 2, 7, 7, 1, 2, 0, 6], 0], [[1, 8, 8, 0, 0, 0, 8, 0, 8, 8, 0, 3, 1, 6], -4], [[2, 9, 9, 1, 1, 0, 8, 0, 0, 9, 1, 0, 0, 6], 0], [[3, 10, 10, 1, 0, 1, 0, 1, 1, 10, 2, 1, 1, 6], -8], [[0, 10, 10, 1, 0, 1, 0, 1, 1, 10, 0, 2, 0, 6], 0], [[2, 2, 1, 3, 2, 3, 2, 3, 3, 12, 2, 1, 1, 6], -4], [[0, 3, 2, 0, 2, 3, 2, 3, 3, 12, 0, 2, 0, 6], 0], [[0, 3, 2, 0, 2, 0, 3, 0, 0, 12, 0, 2, 1, 6], -8], [[0, 1, 0, 1, 3, 1, 1, 2, 2, 1, 0, 0, 0, 6], -4], [[0, 1, 0, 1, 3, 0, 0, 3, 0, 2, 1, 1, 1, 6], -4], [[1, 1, 0, 1, 3, 0, 0, 0, 1, 3, 0, 2, 0, 6], -4], [[1, 1, 0, 1, 0, 1, 1, 1, 1, 3, 0, 2, 1, 6], -4], [[1, 1, 0, 1, 0, 1, 0, 0, 2, 0, 0, 2, 0, 6], 0], [[1, 1, 0, 1, 0, 0, 1, 0, 2, 0, 0, 2, 1, 6], 0], [[1, 1, 0, 1, 0, 0, 0, 1, 2, 0, 0, 2, 0, 6], 0], [[0, 0, 1, 0, 1, 1, 0, 1, 2, 0, 0, 2, 1, 6], 0], [[1, 1, 0, 1, 0, 2, 1, 0, 0, 1, 1, 0, 0, 6], 0], [[1, 1, 0, 0, 1, 2, 1, 0, 0, 1, 1, 0, 1, 6], 0], [[1, 1, 0, 0, 1, 2, 0, 1, 0, 1, 1, 0, 0, 6], 8], [[1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 2, 1, 1, 6], 0], [[1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 2, 1, 0, 6], 0], [[0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 2, 1, 1, 6], 0], [[0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 2, 1, 0, 6], 0], [[0, 0, 0, 0, 2, 1, 0, 1, 0, 1, 2, 1, 1, 6], -8], [[0, 0, 0, 0, 2, 1, 0, 0, 1, 1, 2, 1, 0, 6], -8], [[0, 0, 0, 0, 2, 0, 1, 0, 1, 1, 2, 1, 1, 6], -8], [[0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 2, 1, 0, 6], -8], [[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 1, 1, 6], -8], [[1, 1, 0, 0, 0, 1, 0, 0, 2, 0, 3, 0, 0, 6], -8], [[1, 0, 1, 0, 0, 1, 0, 0, 2, 0, 3, 0, 1, 6], -8]]\n",
      "state_policy_data:  [[[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 6], 3], [[1, 6, 6, 2, 7, 1, 6, 1, 6, 6, 6, 0, 1, 6], 10], [[0, 8, 8, 0, 0, 2, 7, 2, 7, 7, 1, 2, 0, 6], 5], [[1, 8, 8, 0, 0, 0, 8, 0, 8, 8, 0, 3, 1, 6], 8], [[2, 9, 9, 1, 1, 0, 8, 0, 0, 9, 1, 0, 0, 6], 3], [[3, 10, 10, 1, 0, 1, 0, 1, 1, 10, 2, 1, 1, 6], 10], [[0, 10, 10, 1, 0, 1, 0, 1, 1, 10, 0, 2, 0, 6], 1], [[2, 2, 1, 3, 2, 3, 2, 3, 3, 12, 2, 1, 1, 6], 10], [[0, 3, 2, 0, 2, 3, 2, 3, 3, 12, 0, 2, 0, 6], 5], [[0, 3, 2, 0, 2, 0, 3, 0, 0, 12, 0, 2, 1, 6], 6], [[0, 1, 0, 1, 3, 1, 1, 2, 2, 1, 0, 0, 0, 6], 5], [[0, 1, 0, 1, 3, 0, 0, 3, 0, 2, 1, 1, 1, 6], 7], [[1, 1, 0, 1, 3, 0, 0, 0, 1, 3, 0, 2, 0, 6], 4], [[1, 1, 0, 1, 0, 1, 1, 1, 1, 3, 0, 2, 1, 6], 6], [[1, 1, 0, 1, 0, 1, 0, 0, 2, 0, 0, 2, 0, 6], 5], [[1, 1, 0, 1, 0, 0, 1, 0, 2, 0, 0, 2, 1, 6], 6], [[1, 1, 0, 1, 0, 0, 0, 1, 2, 0, 0, 2, 0, 6], 0], [[0, 0, 1, 0, 1, 1, 0, 1, 2, 0, 0, 2, 1, 6], 7], [[1, 1, 0, 1, 0, 2, 1, 0, 0, 1, 1, 0, 0, 6], 3], [[1, 1, 0, 0, 1, 2, 1, 0, 0, 1, 1, 0, 1, 6], 6], [[1, 1, 0, 0, 1, 2, 0, 1, 0, 1, 1, 0, 0, 6], 5], [[1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 2, 1, 1, 6], 6], [[1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 2, 1, 0, 6], 0], [[0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 2, 1, 1, 6], 8], [[0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 2, 1, 0, 6], 2], [[0, 0, 0, 0, 2, 1, 0, 1, 0, 1, 2, 1, 1, 6], 7], [[0, 0, 0, 0, 2, 1, 0, 0, 1, 1, 2, 1, 0, 6], 5], [[0, 0, 0, 0, 2, 0, 1, 0, 1, 1, 2, 1, 1, 6], 6], [[0, 0, 0, 0, 2, 0, 0, 1, 1, 1, 2, 1, 0, 6], 4], [[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 1, 1, 6], 6], [[1, 1, 0, 0, 0, 1, 0, 0, 2, 0, 3, 0, 0, 6], 1], [[1, 0, 1, 0, 0, 1, 0, 0, 2, 0, 3, 0, 1, 6], 8]]\n",
      "0.0 %\n",
      "state_value_data:  [[[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 6], 0], [[6, 6, 0, 1, 6, 6, 2, 7, 1, 6, 1, 6, 0, 6], 4], [[7, 1, 2, 0, 8, 8, 0, 0, 2, 7, 2, 7, 1, 6], -4], [[9, 0, 0, 2, 1, 9, 1, 1, 3, 1, 0, 9, 0, 6], 4], [[10, 1, 1, 2, 1, 0, 2, 2, 0, 2, 1, 10, 1, 6], -8], [[12, 0, 3, 0, 0, 2, 0, 0, 1, 1, 0, 1, 0, 6], 0], [[12, 0, 0, 1, 1, 0, 1, 1, 0, 2, 1, 1, 1, 6], -4], [[1, 0, 2, 0, 3, 2, 0, 2, 0, 2, 0, 0, 0, 6], 4], [[1, 0, 0, 1, 0, 2, 0, 2, 0, 2, 0, 0, 1, 6], 0], [[1, 0, 0, 1, 0, 2, 0, 2, 0, 0, 1, 1, 0, 6], 0], [[0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 2, 1, 6], -8], [[0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 2, 0, 6], -8], [[0, 0, 0, 2, 1, 0, 0, 1, 1, 1, 0, 2, 1, 6], -8], [[0, 0, 0, 2, 1, 0, 0, 1, 1, 0, 1, 2, 0, 6], -8], [[0, 0, 0, 0, 2, 1, 0, 1, 1, 0, 1, 2, 1, 6], -8], [[1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 3, 0, 6], -8], [[1, 0, 0, 0, 0, 2, 1, 0, 0, 1, 0, 3, 1, 6], -8], [[1, 0, 0, 0, 0, 2, 1, 0, 0, 0, 1, 3, 0, 6], -8], [[1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 1, 3, 1, 6], -8], [[1, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 3, 0, 6], -8], [[0, 1, 0, 0, 0, 0, 0, 2, 1, 0, 1, 3, 1, 6], -8], [[0, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 3, 0, 6], -8], [[0, 0, 1, 0, 0, 0, 0, 0, 2, 1, 1, 3, 1, 6], -8]]\n",
      "state_policy_data:  [[[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 6], 6], [[6, 6, 0, 1, 6, 6, 2, 7, 1, 6, 1, 6, 0, 6], 1], [[7, 1, 2, 0, 8, 8, 0, 0, 2, 7, 2, 7, 1, 6], 9], [[9, 0, 0, 2, 1, 9, 1, 1, 3, 1, 0, 9, 0, 6], 5], [[10, 1, 1, 2, 1, 0, 2, 2, 0, 2, 1, 10, 1, 6], 9], [[12, 0, 3, 0, 0, 2, 0, 0, 1, 1, 0, 1, 0, 6], 2], [[12, 0, 0, 1, 1, 0, 1, 1, 0, 2, 1, 1, 1, 6], 6], [[1, 0, 2, 0, 3, 2, 0, 2, 0, 2, 0, 0, 0, 6], 2], [[1, 0, 0, 1, 0, 2, 0, 2, 0, 2, 0, 0, 1, 6], 9], [[1, 0, 0, 1, 0, 2, 0, 2, 0, 0, 1, 1, 0, 6], 5], [[0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 2, 1, 6], 6], [[0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 2, 0, 6], 1], [[0, 0, 0, 2, 1, 0, 0, 1, 1, 1, 0, 2, 1, 6], 9], [[0, 0, 0, 2, 1, 0, 0, 1, 1, 0, 1, 2, 0, 6], 3], [[0, 0, 0, 0, 2, 1, 0, 1, 1, 0, 1, 2, 1, 6], 7], [[1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 0, 3, 0, 6], 4], [[1, 0, 0, 0, 0, 2, 1, 0, 0, 1, 0, 3, 1, 6], 9], [[1, 0, 0, 0, 0, 2, 1, 0, 0, 0, 1, 3, 0, 6], 5], [[1, 0, 0, 0, 0, 0, 2, 1, 0, 0, 1, 3, 1, 6], 6], [[1, 0, 0, 0, 0, 0, 0, 2, 1, 0, 1, 3, 0, 6], 0], [[0, 1, 0, 0, 0, 0, 0, 2, 1, 0, 1, 3, 1, 6], 7], [[0, 1, 0, 0, 0, 0, 0, 0, 2, 1, 1, 3, 0, 6], 1], [[0, 0, 1, 0, 0, 0, 0, 0, 2, 1, 1, 3, 1, 6], 8]]\n",
      "1.0 %\n",
      "state_value_data:  [[[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 6], 0], [[6, 0, 1, 6, 6, 2, 7, 1, 6, 1, 6, 6, 1, 6], -12], [[10, 0, 0, 1, 9, 2, 10, 0, 0, 3, 0, 1, 0, 6], -8], [[10, 0, 0, 1, 9, 0, 11, 1, 0, 3, 0, 1, 1, 6], -12], [[2, 3, 1, 0, 0, 2, 13, 3, 2, 2, 3, 1, 0, 6], 0], [[2, 0, 2, 1, 1, 2, 13, 3, 2, 2, 3, 1, 1, 6], -8], [[2, 0, 2, 1, 1, 2, 13, 0, 3, 3, 0, 1, 0, 6], 4], [[0, 1, 0, 2, 2, 0, 14, 1, 0, 3, 0, 1, 1, 6], -16], [[0, 3, 0, 0, 0, 1, 1, 3, 0, 1, 0, 3, 0, 6], -12], [[0, 0, 1, 1, 1, 1, 1, 3, 0, 1, 0, 3, 1, 6], -12], [[0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 3, 0, 6], -8], [[0, 0, 1, 0, 0, 2, 1, 0, 0, 1, 0, 3, 1, 6], -8], [[0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 1, 3, 0, 6], -8], [[0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 1, 3, 1, 6], -8], [[0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 1, 3, 0, 6], -8], [[0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 1, 3, 1, 6], -8], [[0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 1, 3, 0, 6], -8], [[0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 3, 1, 6], -8], [[0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 1, 3, 0, 6], -8], [[0, 0, 0, 0, 0, 1, 0, 0, 2, 1, 1, 3, 1, 6], -8]]\n",
      "state_policy_data:  [[[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 6], 5], [[6, 0, 1, 6, 6, 2, 7, 1, 6, 1, 6, 6, 1, 6], 9], [[10, 0, 0, 1, 9, 2, 10, 0, 0, 3, 0, 1, 0, 6], 5], [[10, 0, 0, 1, 9, 0, 11, 1, 0, 3, 0, 1, 1, 6], 9], [[2, 3, 1, 0, 0, 2, 13, 3, 2, 2, 3, 1, 0, 6], 1], [[2, 0, 2, 1, 1, 2, 13, 3, 2, 2, 3, 1, 1, 6], 7], [[2, 0, 2, 1, 1, 2, 13, 0, 3, 3, 0, 1, 0, 6], 0], [[0, 1, 0, 2, 2, 0, 14, 1, 0, 3, 0, 1, 1, 6], 6], [[0, 3, 0, 0, 0, 1, 1, 3, 0, 1, 0, 3, 0, 6], 1], [[0, 0, 1, 1, 1, 1, 1, 3, 0, 1, 0, 3, 1, 6], 6], [[0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 3, 0, 6], 3], [[0, 0, 1, 0, 0, 2, 1, 0, 0, 1, 0, 3, 1, 6], 9], [[0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 1, 3, 0, 6], 2], [[0, 0, 0, 1, 0, 2, 1, 0, 0, 0, 1, 3, 1, 6], 6], [[0, 0, 0, 1, 0, 2, 0, 1, 0, 0, 1, 3, 0, 6], 3], [[0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 1, 3, 1, 6], 7], [[0, 0, 0, 0, 1, 2, 0, 0, 1, 0, 1, 3, 0, 6], 5], [[0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 3, 1, 6], 6], [[0, 0, 0, 0, 1, 0, 0, 0, 2, 1, 1, 3, 0, 6], 4], [[0, 0, 0, 0, 0, 1, 0, 0, 2, 1, 1, 3, 1, 6], 8]]\n",
      "2.0 %\n"
     ]
    }
   ],
   "source": [
    "value_data, policy_data = generate_training_data()\n",
    "len(value_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.2765 - loss: 34.7690 - val_accuracy: 0.2227 - val_loss: 703.0892 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2632 - loss: 28.0122 - val_accuracy: 0.2227 - val_loss: 700.8851 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2591 - loss: 26.5041 - val_accuracy: 0.2227 - val_loss: 698.2056 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2856 - loss: 24.0992 - val_accuracy: 0.2227 - val_loss: 695.6357 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.3035 - loss: 20.4567 - val_accuracy: 0.2185 - val_loss: 693.7120 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2495 - loss: 18.3316 - val_accuracy: 0.2185 - val_loss: 693.1804 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2512 - loss: 15.9919 - val_accuracy: 0.2164 - val_loss: 693.0582 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2595 - loss: 16.0929 - val_accuracy: 0.1933 - val_loss: 692.4198 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2270 - loss: 14.1915 - val_accuracy: 0.1639 - val_loss: 692.3428 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2095 - loss: 13.2889 - val_accuracy: 0.1702 - val_loss: 691.8012 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2004 - loss: 13.4263 - val_accuracy: 0.1723 - val_loss: 691.1135 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2088 - loss: 13.7433 - val_accuracy: 0.1723 - val_loss: 691.1025 - learning_rate: 1.0000e-05\n",
      "Epoch 13/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2147 - loss: 13.9670 - val_accuracy: 0.1723 - val_loss: 691.0909 - learning_rate: 1.0000e-05\n",
      "Epoch 14/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2124 - loss: 13.3351 - val_accuracy: 0.1723 - val_loss: 691.0790 - learning_rate: 1.0000e-05\n",
      "Epoch 15/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2131 - loss: 12.8949 - val_accuracy: 0.1723 - val_loss: 691.0745 - learning_rate: 1.0000e-05\n",
      "Epoch 16/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1974 - loss: 13.2252 - val_accuracy: 0.1723 - val_loss: 691.0688 - learning_rate: 1.0000e-05\n",
      "Epoch 17/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1980 - loss: 13.2250 - val_accuracy: 0.1723 - val_loss: 691.0613 - learning_rate: 1.0000e-05\n",
      "Epoch 18/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2122 - loss: 12.6422 - val_accuracy: 0.1723 - val_loss: 691.0544 - learning_rate: 1.0000e-05\n",
      "Epoch 19/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2211 - loss: 12.6975 - val_accuracy: 0.1723 - val_loss: 691.0519 - learning_rate: 1.0000e-05\n",
      "Epoch 20/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2244 - loss: 13.2661 - val_accuracy: 0.1723 - val_loss: 691.0477 - learning_rate: 1.0000e-05\n",
      "Epoch 21/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2038 - loss: 12.5979 - val_accuracy: 0.1723 - val_loss: 691.0439 - learning_rate: 1.0000e-05\n",
      "Epoch 22/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2154 - loss: 13.1395 - val_accuracy: 0.1723 - val_loss: 691.0439 - learning_rate: 1.0000e-07\n",
      "Epoch 23/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.2236 - loss: 13.6155 - val_accuracy: 0.1723 - val_loss: 691.0437 - learning_rate: 1.0000e-07\n",
      "Epoch 24/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.2177 - loss: 13.1479 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-07\n",
      "Epoch 25/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.2193 - loss: 12.1530 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-07\n",
      "Epoch 26/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2188 - loss: 12.3121 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-07\n",
      "Epoch 27/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2124 - loss: 12.6299 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-07\n",
      "Epoch 28/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1974 - loss: 12.8361 - val_accuracy: 0.1723 - val_loss: 691.0435 - learning_rate: 1.0000e-07\n",
      "Epoch 29/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2223 - loss: 13.8660 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-07\n",
      "Epoch 30/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2131 - loss: 13.2853 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-07\n",
      "Epoch 31/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2004 - loss: 12.3684 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-07\n",
      "Epoch 32/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2085 - loss: 12.4120 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-09\n",
      "Epoch 33/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2187 - loss: 12.7910 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-09\n",
      "Epoch 34/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2038 - loss: 12.4972 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-09\n",
      "Epoch 35/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2065 - loss: 12.8084 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-09\n",
      "Epoch 36/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2102 - loss: 13.9780 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-09\n",
      "Epoch 37/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1999 - loss: 13.4973 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-09\n",
      "Epoch 38/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1919 - loss: 13.0431 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-09\n",
      "Epoch 39/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2166 - loss: 13.4660 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-09\n",
      "Epoch 40/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2102 - loss: 12.4982 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-09\n",
      "Epoch 41/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2220 - loss: 13.1230 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-09\n",
      "Epoch 42/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2243 - loss: 13.7622 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-11\n",
      "Epoch 43/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1980 - loss: 13.2917 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-11\n",
      "Epoch 44/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2186 - loss: 13.3055 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-11\n",
      "Epoch 45/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2085 - loss: 12.8050 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-11\n",
      "Epoch 46/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2011 - loss: 13.5346 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-11\n",
      "Epoch 47/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2213 - loss: 12.6187 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-11\n",
      "Epoch 48/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2149 - loss: 13.3041 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-11\n",
      "Epoch 49/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2267 - loss: 13.4812 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-11\n",
      "Epoch 50/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2226 - loss: 11.8910 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-11\n",
      "Epoch 51/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2101 - loss: 12.5518 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-11\n",
      "Epoch 52/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2225 - loss: 13.3779 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-13\n",
      "Epoch 53/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2093 - loss: 14.2329 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-13\n",
      "Epoch 54/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1880 - loss: 12.9011 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-13\n",
      "Epoch 55/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2080 - loss: 12.8356 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-13\n",
      "Epoch 56/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2180 - loss: 13.6525 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-13\n",
      "Epoch 57/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2199 - loss: 12.4194 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-13\n",
      "Epoch 58/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2061 - loss: 14.0618 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-13\n",
      "Epoch 59/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2080 - loss: 12.8157 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-13\n",
      "Epoch 60/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2096 - loss: 12.6093 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-13\n",
      "Epoch 61/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2119 - loss: 12.6388 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-13\n",
      "Epoch 62/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2252 - loss: 13.0034 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-15\n",
      "Epoch 63/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2109 - loss: 12.8360 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-15\n",
      "Epoch 64/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2240 - loss: 12.5951 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-15\n",
      "Epoch 65/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.2376 - loss: 12.5181 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-15\n",
      "Epoch 66/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2058 - loss: 13.4366 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-15\n",
      "Epoch 67/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2067 - loss: 13.0538 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-15\n",
      "Epoch 68/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2181 - loss: 12.8013 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-15\n",
      "Epoch 69/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2230 - loss: 12.0353 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-15\n",
      "Epoch 70/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1985 - loss: 13.2929 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-15\n",
      "Epoch 71/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2254 - loss: 12.7881 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-15\n",
      "Epoch 72/200\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1994 - loss: 13.4144 - val_accuracy: 0.1723 - val_loss: 691.0436 - learning_rate: 1.0000e-17\n"
     ]
    }
   ],
   "source": [
    "value_model.train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.4147128 ],\n",
       "       [-0.95667267],\n",
       "       [ 2.140177  ],\n",
       "       [-5.8980227 ],\n",
       "       [-2.7277    ]], dtype=float32)"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# play games\n",
    "# store outcome of games\n",
    "# train model\n",
    "data = [\n",
    "    [6, 2, 7, 1, 6, 1, 6, 6, 6, 0, 1, 6, 1, 6],# -8\n",
    "    [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 6],# 0\n",
    "    [3, 1, 3, 0, 2, 1, 0, 2, 2, 0, 2, 0, 0, 6],# 0\n",
    "    [0, 0, 0, 0, 1, 1, 0, 1, 3, 1, 0, 1, 1, 6],# -8\n",
    "    [2, 0, 3, 1, 3, 1, 2, 2, 1, 2, 3, 12, 1, 6],# -8\n",
    "]\n",
    "# data = [[0, 0, 3, 11, 1, 1, 0, 1, 2, 11, 1, 1, 1, 6]]\n",
    "value_model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = 'checkpoint.model.keras'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "# Model is saved at the end of every epoch, if it's the best seen so far.\n",
    "model.fit(epochs=EPOCHS, callbacks=[model_checkpoint_callback])\n",
    "\n",
    "# The model (that are considered the best) can be loaded as -\n",
    "keras.models.load_model(checkpoint_filepath)\n",
    "\n",
    "# Alternatively, one could checkpoint just the model weights as -\n",
    "checkpoint_filepath = '/tmp/ckpt/checkpoint.weights.h5'\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "# Model weights are saved at the end of every epoch, if it's the best seen\n",
    "# so far.\n",
    "model.fit(epochs=EPOCHS, callbacks=[model_checkpoint_callback])\n",
    "\n",
    "# The model weights (that are considered the best) can be loaded as -\n",
    "model.load_weights(checkpoint_filepath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
