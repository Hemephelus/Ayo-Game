{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "import copy\n",
    "import random\n",
    "from json.encoder import INFINITY\n",
    "import pandas as pd\n",
    "import math\n",
    "from ayo_game import play, is_illegal_move, assign_reward, print_game_play, end_game\n",
    "from agents import random_agent as ra\n",
    "from agents import minimax_agent as ma\n",
    "from agents import mcts_agent as mctsa\n",
    "seed = 37\n",
    "\n",
    "randint = random.randint\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_agent = ra.agent\n",
    "minimax_agent = ma.agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyModel:\n",
    "    def __init__(self):\n",
    "        self.num_actions=12\n",
    "        self.X_shape=14\n",
    "        self.model = Sequential([\n",
    "            Dense(64, input_dim=self.X_shape, activation='relu'),  # First hidden layer\n",
    "            Dense(32, activation='relu'),  # Second hidden layer\n",
    "            Dense(self.num_actions, activation='softmax')  # Output layer for 12 possible actions\n",
    "        ])\n",
    "\n",
    "        # Compile the model\n",
    "        self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    def train(self, X,y):\n",
    "        # Convert target (y) to categorical (for classification)\n",
    "        num_actions = self.num_actions # Assuming 12 possible actions (0 to 11)\n",
    "        y = tf.keras.utils.to_categorical(y, num_classes=num_actions)\n",
    "\n",
    "        # Train the model (Assume more training data is available)\n",
    "        # For example purposes, we'll use the same sample data multiple times\n",
    "        self.model.fit(X, y, epochs=10, batch_size=1)\n",
    "\n",
    "\n",
    "    def predict(self,data):\n",
    "        prediction = self.model.predict(data)\n",
    "        return prediction\n",
    "    \n",
    "\n",
    "class ValueModel:\n",
    "    def __init__(self):\n",
    "        self.num_actions=12\n",
    "        self.X_shape=14\n",
    "        self.model = Sequential([\n",
    "            Dense(64, input_dim=self.X_shape, activation='relu'),  # First hidden layer\n",
    "            Dense(32, activation='relu'),  # Second hidden layer\n",
    "            Dense(1)  # Output layer for 12 possible actions\n",
    "        ])\n",
    "\n",
    "        # Compile the model\n",
    "        self.model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    def train(self, X,y):\n",
    "        # Train the model (Assume more training data is available)\n",
    "        # For example purposes, we'll use the same sample data multiple times\n",
    "        self.model.fit(X, y, epochs=10, batch_size=1)\n",
    "\n",
    "\n",
    "    def predict(self,data):\n",
    "        # Predict action for a new data point\n",
    "        prediction = self.model.predict(data)\n",
    "        return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_episode(state,  agent_1, agent_2,show=False):\n",
    "    state = copy.deepcopy(state)\n",
    "    reward = [0,0,0]\n",
    "    path = []\n",
    "\n",
    "    while True:\n",
    "        action = generate_action(state, agent_1, agent_2)\n",
    "\n",
    "        if is_illegal_move(state, action):\n",
    "            continue\n",
    "\n",
    "        state, new_reward = play(state, action)\n",
    "        reward = assign_reward(reward, new_reward)\n",
    "        if show:\n",
    "            print_game_play(state, reward, action)\n",
    "\n",
    "        if end_game(state):\n",
    "            break\n",
    "        path.append(action)\n",
    "\n",
    "    return  (reward, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lol():\n",
    "    num_of_eps = 10\n",
    "    data = []\n",
    "    state = {\n",
    "   # 'board' :[6, 6, 2, 7, 1, 6, 1, 6, 6, 6, 0, 1],\n",
    "   'board' :[4,4,4,4,4,4,4,4,4,4,4,4],\n",
    "   'current_player': 0,\n",
    "   'player_territory': (0,6)\n",
    "}\n",
    "    \n",
    "    agent_1 = random_agent_details = {\n",
    "    'func': random_agent,\n",
    "    'arg': {},\n",
    "    'name': 'random_agent',\n",
    "    'elo': 1200\n",
    "    }\n",
    "        \n",
    "    minimax_agent_details_3 = {\n",
    "    'func': minimax_agent,\n",
    "    'arg': {\n",
    "        'max_dept': 3,\n",
    "    },\n",
    "    'name': 'minimax_agent_dept_3',\n",
    "    'elo': 1200\n",
    "    }\n",
    "    for _ in range(num_of_eps):\n",
    "        training_examples = execute_episode(state,  agent_1, agent_2)\n",
    "        data.extend(training_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play games\n",
    "# store outcome of games\n",
    "# train model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
